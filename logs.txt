
==> Audit <==
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| docker-env | --shell cmd                    | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 30 Aug 24 14:34 CDT |                     |
| start      | --driver=docker                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 06:56 CDT | 31 Aug 24 07:03 CDT |
| image      | ls                             | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 07:10 CDT | 31 Aug 24 07:10 CDT |
| docker-env |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 07:14 CDT | 31 Aug 24 07:14 CDT |
| start      | --driver=docker                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 10:54 CDT |                     |
| start      | --driver=docker                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 10:56 CDT | 31 Aug 24 10:59 CDT |
| docker-env |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 10:59 CDT | 31 Aug 24 10:59 CDT |
| image      | ls                             | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:04 CDT | 31 Aug 24 11:04 CDT |
| image      | load customer-model-app        | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:10 CDT |                     |
| image      | load customer-model-app        | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:12 CDT |                     |
| image      | load customer-model-app:latest | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:14 CDT | 31 Aug 24 11:15 CDT |
| image      | load customer-model-app:latest | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:17 CDT | 31 Aug 24 11:19 CDT |
| image      | ls                             | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:20 CDT | 31 Aug 24 11:20 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 11:29 CDT | 31 Aug 24 11:43 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 12:00 CDT | 31 Aug 24 12:03 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 12:11 CDT | 31 Aug 24 12:16 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 12:42 CDT | 31 Aug 24 12:46 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 12:46 CDT | 31 Aug 24 12:51 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 12:53 CDT | 31 Aug 24 12:56 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 13:02 CDT | 31 Aug 24 13:10 CDT |
| ip         |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 13:10 CDT | 31 Aug 24 13:11 CDT |
| ip         |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 13:15 CDT | 31 Aug 24 13:15 CDT |
| service    | postgres --url                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 13:17 CDT | 31 Aug 24 13:22 CDT |
| service    | postgres                       | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 13:22 CDT | 31 Aug 24 14:12 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 14:55 CDT | 31 Aug 24 15:06 CDT |
| service    | postgres                       | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 14:57 CDT | 31 Aug 24 15:09 CDT |
| service    | customer-model                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:07 CDT | 31 Aug 24 15:08 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:10 CDT | 31 Aug 24 15:12 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:28 CDT | 31 Aug 24 15:31 CDT |
| ssh        | docker@minikube                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:30 CDT |                     |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:31 CDT | 31 Aug 24 15:40 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:35 CDT |                     |
| ssh        |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:37 CDT | 31 Aug 24 15:38 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:40 CDT | 31 Aug 24 15:44 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:44 CDT | 31 Aug 24 15:45 CDT |
| ssh        |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:45 CDT | 31 Aug 24 15:46 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:46 CDT | 31 Aug 24 15:49 CDT |
| service    | postgres                       | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:48 CDT | 31 Aug 24 15:53 CDT |
| tunnel     |                                | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:49 CDT | 31 Aug 24 15:52 CDT |
| service    | customer-model                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:54 CDT |                     |
| service    | customer-model                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 15:57 CDT |                     |
| service    | customer-model                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 16:24 CDT |                     |
| image      | ls                             | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 16:46 CDT | 31 Aug 24 16:46 CDT |
| service    | customer-model                 | minikube | DESKTOP-B2A4O3B\Nitya | v1.33.1 | 31 Aug 24 16:47 CDT |                     |
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/08/31 10:56:04
Running on machine: DESKTOP-B2A4O3B
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0831 10:56:04.195494   19320 out.go:291] Setting OutFile to fd 84 ...
I0831 10:56:04.196495   19320 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0831 10:56:04.196495   19320 out.go:304] Setting ErrFile to fd 88...
I0831 10:56:04.196495   19320 out.go:338] TERM=,COLORTERM=, which probably does not support color
W0831 10:56:04.210205   19320 root.go:314] Error reading config file at C:\Users\Venka\.minikube\config\config.json: open C:\Users\Venka\.minikube\config\config.json: The system cannot find the file specified.
I0831 10:56:04.215824   19320 out.go:298] Setting JSON to false
I0831 10:56:04.220749   19320 start.go:129] hostinfo: {"hostname":"DESKTOP-B2A4O3B","uptime":120622,"bootTime":1724999141,"procs":319,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4780 Build 19045.4780","kernelVersion":"10.0.19045.4780 Build 19045.4780","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0b8f46ff-4d93-4ca4-93a9-f9091abb3a3b"}
W0831 10:56:04.221260   19320 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0831 10:56:04.384214   19320 out.go:177] * minikube v1.33.1 on Microsoft Windows 10 Pro 10.0.19045.4780 Build 19045.4780
I0831 10:56:04.768097   19320 notify.go:220] Checking for updates...
I0831 10:56:04.769193   19320 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0831 10:56:04.878657   19320 driver.go:392] Setting default libvirt URI to qemu:///system
I0831 10:56:05.365525   19320 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.28.0 (139021)
I0831 10:56:05.370894   19320 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 10:56:07.865389   19320 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.4939884s)
I0831 10:56:07.865417   19320 info.go:266] docker info: {ID:db020ee8-4be7-421d-b82c-2aa173879ba5 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:70 OomKillDisable:true NGoroutines:133 SystemTime:2024-08-31 15:56:07.750112498 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8267005952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0831 10:56:08.122524   19320 out.go:177] * Using the docker driver based on existing profile
I0831 10:56:08.503135   19320 start.go:297] selected driver: docker
I0831 10:56:08.525125   19320 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Venka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0831 10:56:08.525125   19320 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0831 10:56:08.994120   19320 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0831 10:56:13.481638   19320 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.4875184s)
I0831 10:56:13.482673   19320 info.go:266] docker info: {ID:db020ee8-4be7-421d-b82c-2aa173879ba5 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:70 OomKillDisable:true NGoroutines:133 SystemTime:2024-08-31 15:56:13.113675736 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8267005952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.22] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.5.0]] Warnings:<nil>}}
I0831 10:56:16.643192   19320 cni.go:84] Creating CNI manager for ""
I0831 10:56:17.174197   19320 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0831 10:56:17.451778   19320 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Venka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0831 10:56:18.007176   19320 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0831 10:56:18.803767   19320 cache.go:121] Beginning downloading kic base image for docker with docker
I0831 10:56:19.280469   19320 out.go:177] * Pulling base image v0.0.44 ...
I0831 10:56:19.851149   19320 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0831 10:56:19.857555   19320 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0831 10:56:19.881643   19320 preload.go:147] Found local preload: C:\Users\Venka\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0831 10:56:19.919956   19320 cache.go:56] Caching tarball of preloaded images
I0831 10:56:19.986257   19320 preload.go:173] Found C:\Users\Venka\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0831 10:56:19.986903   19320 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0831 10:56:20.021810   19320 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0831 10:56:20.021810   19320 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0831 10:56:20.076073   19320 profile.go:143] Saving config to C:\Users\Venka\.minikube\profiles\minikube\config.json ...
I0831 10:56:20.107214   19320 cache.go:194] Successfully downloaded all kic artifacts
I0831 10:56:20.132196   19320 start.go:360] acquireMachinesLock for minikube: {Name:mk99f61b3f6939e395ce6aa74cbbd7312a04bb8d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0831 10:56:20.132196   19320 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0831 10:56:20.132196   19320 start.go:96] Skipping create...Using existing machine configuration
I0831 10:56:20.135200   19320 fix.go:54] fixHost starting: 
I0831 10:56:20.148285   19320 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 10:56:20.288195   19320 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0831 10:56:20.288195   19320 fix.go:138] unexpected machine state, will restart: <nil>
I0831 10:56:20.347429   19320 out.go:177] * Updating the running docker "minikube" container ...
I0831 10:56:20.390366   19320 machine.go:94] provisionDockerMachine start ...
I0831 10:56:20.418452   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:20.559553   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:20.817933   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:20.817933   19320 main.go:141] libmachine: About to run SSH command:
hostname
I0831 10:56:21.238914   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0831 10:56:21.238914   19320 ubuntu.go:169] provisioning hostname "minikube"
I0831 10:56:21.251873   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:21.411323   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:21.411323   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:21.411323   19320 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0831 10:56:21.573020   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0831 10:56:21.636518   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:21.797695   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:21.797695   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:21.797695   19320 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0831 10:56:22.118997   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0831 10:56:22.129521   19320 ubuntu.go:175] set auth options {CertDir:C:\Users\Venka\.minikube CaCertPath:C:\Users\Venka\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Venka\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Venka\.minikube\machines\server.pem ServerKeyPath:C:\Users\Venka\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Venka\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Venka\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Venka\.minikube}
I0831 10:56:22.129521   19320 ubuntu.go:177] setting up certificates
I0831 10:56:22.129521   19320 provision.go:84] configureAuth start
I0831 10:56:22.165350   19320 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 10:56:22.339907   19320 provision.go:143] copyHostCerts
I0831 10:56:22.523809   19320 exec_runner.go:144] found C:\Users\Venka\.minikube/ca.pem, removing ...
I0831 10:56:22.528810   19320 exec_runner.go:203] rm: C:\Users\Venka\.minikube\ca.pem
I0831 10:56:22.528810   19320 exec_runner.go:151] cp: C:\Users\Venka\.minikube\certs\ca.pem --> C:\Users\Venka\.minikube/ca.pem (1074 bytes)
I0831 10:56:22.706435   19320 exec_runner.go:144] found C:\Users\Venka\.minikube/cert.pem, removing ...
I0831 10:56:22.707242   19320 exec_runner.go:203] rm: C:\Users\Venka\.minikube\cert.pem
I0831 10:56:22.707292   19320 exec_runner.go:151] cp: C:\Users\Venka\.minikube\certs\cert.pem --> C:\Users\Venka\.minikube/cert.pem (1119 bytes)
I0831 10:56:22.774031   19320 exec_runner.go:144] found C:\Users\Venka\.minikube/key.pem, removing ...
I0831 10:56:22.774088   19320 exec_runner.go:203] rm: C:\Users\Venka\.minikube\key.pem
I0831 10:56:22.774597   19320 exec_runner.go:151] cp: C:\Users\Venka\.minikube\certs\key.pem --> C:\Users\Venka\.minikube/key.pem (1675 bytes)
I0831 10:56:22.775161   19320 provision.go:117] generating server cert: C:\Users\Venka\.minikube\machines\server.pem ca-key=C:\Users\Venka\.minikube\certs\ca.pem private-key=C:\Users\Venka\.minikube\certs\ca-key.pem org=Nitya.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0831 10:56:24.913345   19320 provision.go:177] copyRemoteCerts
I0831 10:56:24.921346   19320 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0831 10:56:24.930189   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:25.084441   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
I0831 10:56:25.245071   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0831 10:56:25.276619   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0831 10:56:25.304553   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0831 10:56:25.352828   19320 provision.go:87] duration metric: took 3.2080484s to configureAuth
I0831 10:56:25.352828   19320 ubuntu.go:193] setting minikube options for container-runtime
I0831 10:56:25.352859   19320 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0831 10:56:25.358664   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:25.506546   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:25.507087   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:25.507087   19320 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0831 10:56:25.680858   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0831 10:56:25.697422   19320 ubuntu.go:71] root file system type: overlay
I0831 10:56:25.737799   19320 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0831 10:56:25.744244   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:25.941699   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:25.941699   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:25.941699   19320 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0831 10:56:26.116392   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0831 10:56:26.149694   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:26.403878   19320 main.go:141] libmachine: Using SSH client type: native
I0831 10:56:26.403878   19320 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xaaa3c0] 0xaacfa0 <nil>  [] 0s} 127.0.0.1 58422 <nil> <nil>}
I0831 10:56:26.403878   19320 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0831 10:56:26.564472   19320 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0831 10:56:26.564472   19320 machine.go:97] duration metric: took 6.1741061s to provisionDockerMachine
I0831 10:56:26.564472   19320 start.go:293] postStartSetup for "minikube" (driver="docker")
I0831 10:56:26.564472   19320 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0831 10:56:26.574699   19320 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0831 10:56:26.580110   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:26.728575   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
I0831 10:56:26.842249   19320 ssh_runner.go:195] Run: cat /etc/os-release
I0831 10:56:26.848636   19320 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0831 10:56:26.848665   19320 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0831 10:56:26.848665   19320 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0831 10:56:26.848665   19320 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0831 10:56:26.848665   19320 filesync.go:126] Scanning C:\Users\Venka\.minikube\addons for local assets ...
I0831 10:56:26.849206   19320 filesync.go:126] Scanning C:\Users\Venka\.minikube\files for local assets ...
I0831 10:56:26.849206   19320 start.go:296] duration metric: took 284.7332ms for postStartSetup
I0831 10:56:26.856931   19320 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0831 10:56:26.862507   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:27.004841   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
I0831 10:56:27.127562   19320 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0831 10:56:27.386728   19320 fix.go:56] duration metric: took 7.2515284s for fixHost
I0831 10:56:27.386728   19320 start.go:83] releasing machines lock for "minikube", held for 7.2545317s
I0831 10:56:27.392730   19320 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0831 10:56:28.433753   19320 cli_runner.go:217] Completed: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube: (1.0410227s)
I0831 10:56:28.435938   19320 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0831 10:56:28.442021   19320 ssh_runner.go:195] Run: cat /version.json
I0831 10:56:28.520089   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:28.691815   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:56:28.869532   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
I0831 10:56:29.027044   19320 ssh_runner.go:195] Run: systemctl --version
I0831 10:56:29.047381   19320 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0831 10:56:29.134259   19320 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0831 10:56:29.163171   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
W0831 10:56:29.349058   19320 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0831 10:56:29.394867   19320 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0831 10:56:29.593837   19320 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0831 10:56:29.593837   19320 start.go:494] detecting cgroup driver to use...
I0831 10:56:29.593837   19320 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.1578986s)
I0831 10:56:29.593837   19320 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0831 10:56:30.016036   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0831 10:56:30.110391   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0831 10:56:30.160777   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0831 10:56:30.179184   19320 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0831 10:56:30.191991   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0831 10:56:30.221466   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0831 10:56:30.253789   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0831 10:56:30.284777   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0831 10:56:30.319691   19320 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0831 10:56:30.348368   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0831 10:56:30.370110   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0831 10:56:30.394774   19320 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0831 10:56:30.441524   19320 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0831 10:56:30.462009   19320 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0831 10:56:30.480669   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:56:30.686678   19320 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0831 10:56:45.352218   19320 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (14.6655404s)
I0831 10:56:45.352218   19320 start.go:494] detecting cgroup driver to use...
I0831 10:56:45.352218   19320 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0831 10:56:45.368164   19320 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0831 10:56:45.385079   19320 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0831 10:56:45.397435   19320 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0831 10:56:45.420315   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0831 10:56:45.468388   19320 ssh_runner.go:195] Run: which cri-dockerd
I0831 10:56:45.667638   19320 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0831 10:56:45.681229   19320 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0831 10:56:45.710217   19320 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0831 10:56:45.850195   19320 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0831 10:56:45.970721   19320 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0831 10:56:45.989956   19320 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0831 10:56:46.025088   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:56:46.158457   19320 ssh_runner.go:195] Run: sudo systemctl restart docker
I0831 10:57:02.819052   19320 ssh_runner.go:235] Completed: sudo systemctl restart docker: (16.6605957s)
I0831 10:57:02.842096   19320 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0831 10:57:03.360220   19320 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0831 10:57:03.403198   19320 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0831 10:57:03.434706   19320 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0831 10:57:03.578011   19320 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0831 10:57:03.696207   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:57:03.830795   19320 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0831 10:57:03.854360   19320 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0831 10:57:03.876787   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:57:04.009262   19320 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0831 10:57:04.481015   19320 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0831 10:57:04.603724   19320 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0831 10:57:04.609976   19320 start.go:562] Will wait 60s for crictl version
I0831 10:57:04.617130   19320 ssh_runner.go:195] Run: which crictl
I0831 10:57:04.630374   19320 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0831 10:57:04.672156   19320 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0831 10:57:04.680811   19320 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 10:57:04.714595   19320 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0831 10:57:04.885255   19320 out.go:204] * Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0831 10:57:05.374082   19320 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0831 10:57:08.623579   19320 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (3.2494968s)
I0831 10:57:08.623579   19320 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0831 10:57:08.631247   19320 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0831 10:57:08.642343   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 10:57:09.694234   19320 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Venka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0831 10:57:09.707243   19320 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0831 10:57:09.713656   19320 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 10:57:09.737212   19320 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 10:57:09.737212   19320 docker.go:615] Images already preloaded, skipping extraction
I0831 10:57:09.743193   19320 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0831 10:57:09.763625   19320 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0831 10:57:09.764132   19320 cache_images.go:84] Images are preloaded, skipping loading
I0831 10:57:09.764161   19320 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0831 10:57:09.940876   19320 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0831 10:57:09.946217   19320 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0831 10:57:10.549937   19320 cni.go:84] Creating CNI manager for ""
I0831 10:57:10.549937   19320 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0831 10:57:10.559939   19320 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0831 10:57:10.559939   19320 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0831 10:57:10.559939   19320 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0831 10:57:10.567515   19320 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0831 10:57:10.578323   19320 binaries.go:44] Found k8s binaries, skipping transfer
I0831 10:57:10.585915   19320 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0831 10:57:10.596127   19320 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0831 10:57:10.615507   19320 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0831 10:57:10.636260   19320 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0831 10:57:10.664120   19320 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0831 10:57:10.677811   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:57:15.958336   19320 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (5.2805254s)
I0831 10:57:15.965970   19320 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0831 10:57:16.540530   19320 certs.go:68] Setting up C:\Users\Venka\.minikube\profiles\minikube for IP: 192.168.49.2
I0831 10:57:16.540530   19320 certs.go:194] generating shared ca certs ...
I0831 10:57:16.540530   19320 certs.go:226] acquiring lock for ca certs: {Name:mk5b54c0f57f7a7cb6a9ce187d5f100e838cbbb2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 10:57:16.540530   19320 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Venka\.minikube\ca.key
I0831 10:57:16.617226   19320 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Venka\.minikube\proxy-client-ca.key
I0831 10:57:16.708345   19320 certs.go:256] generating profile certs ...
I0831 10:57:16.709349   19320 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\Venka\.minikube\profiles\minikube\client.key
I0831 10:57:16.815992   19320 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\Venka\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0831 10:57:17.325975   19320 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\Venka\.minikube\profiles\minikube\proxy-client.key
I0831 10:57:17.479385   19320 certs.go:484] found cert: C:\Users\Venka\.minikube\certs\ca-key.pem (1675 bytes)
I0831 10:57:17.479532   19320 certs.go:484] found cert: C:\Users\Venka\.minikube\certs\ca.pem (1074 bytes)
I0831 10:57:17.479532   19320 certs.go:484] found cert: C:\Users\Venka\.minikube\certs\cert.pem (1119 bytes)
I0831 10:57:17.479532   19320 certs.go:484] found cert: C:\Users\Venka\.minikube\certs\key.pem (1675 bytes)
I0831 10:57:20.182761   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0831 10:57:20.210467   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0831 10:57:20.561279   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0831 10:57:20.590295   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0831 10:57:20.729183   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0831 10:57:20.798529   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0831 10:57:20.991939   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0831 10:57:21.018641   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0831 10:57:21.112669   19320 ssh_runner.go:362] scp C:\Users\Venka\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0831 10:57:21.166608   19320 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0831 10:57:21.234568   19320 ssh_runner.go:195] Run: openssl version
I0831 10:57:21.266628   19320 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0831 10:57:21.531412   19320 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0831 10:57:21.537719   19320 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug 31 12:02 /usr/share/ca-certificates/minikubeCA.pem
I0831 10:57:21.545783   19320 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0831 10:57:21.561970   19320 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0831 10:57:21.581097   19320 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0831 10:57:21.594615   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0831 10:57:21.676149   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0831 10:57:21.692663   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0831 10:57:21.709010   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0831 10:57:21.725204   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0831 10:57:21.740832   19320 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0831 10:57:21.749238   19320 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Venka:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0831 10:57:21.755003   19320 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0831 10:57:21.784028   19320 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0831 10:57:21.796416   19320 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0831 10:57:21.796416   19320 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0831 10:57:21.796416   19320 kubeadm.go:587] restartPrimaryControlPlane start ...
I0831 10:57:21.805181   19320 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0831 10:57:21.817700   19320 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0831 10:57:21.822938   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 10:57:22.538111   19320 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:58426"
I0831 10:57:27.905089   19320 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0831 10:57:27.916311   19320 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0831 10:57:27.916322   19320 kubeadm.go:591] duration metric: took 6.1199055s to restartPrimaryControlPlane
I0831 10:57:27.916322   19320 kubeadm.go:393] duration metric: took 6.1670839s to StartCluster
I0831 10:57:27.916322   19320 settings.go:142] acquiring lock: {Name:mke8ff32cd3ec911aab2f63c7fa7d090004fbb74 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 10:57:27.916322   19320 settings.go:150] Updating kubeconfig:  C:\Users\Venka\.kube\config
I0831 10:57:27.917422   19320 lock.go:35] WriteFile acquiring C:\Users\Venka\.kube\config: {Name:mk08efdea4dd43ab7d0a9a052d98e4ad06df1954 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0831 10:57:28.001792   19320 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0831 10:57:28.086817   19320 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0831 10:57:28.086817   19320 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0831 10:57:28.086817   19320 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0831 10:57:28.136801   19320 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0831 10:57:28.136801   19320 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0831 10:57:28.136801   19320 addons.go:243] addon storage-provisioner should already be in state true
I0831 10:57:28.136801   19320 host.go:66] Checking if "minikube" exists ...
I0831 10:57:28.236411   19320 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0831 10:57:28.406282   19320 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 10:57:28.406282   19320 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 10:57:28.480036   19320 out.go:177] * Verifying Kubernetes components...
I0831 10:57:28.631607   19320 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0831 10:57:29.055776   19320 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
W0831 10:57:28.808976   19320 addons.go:243] addon default-storageclass should already be in state true
I0831 10:57:28.816174   19320 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0831 10:57:29.056023   19320 host.go:66] Checking if "minikube" exists ...
I0831 10:57:29.189604   19320 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0831 10:57:29.239039   19320 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0831 10:57:29.411719   19320 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0831 10:57:29.266273   19320 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0831 10:57:29.281067   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0831 10:57:29.421255   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:57:29.586662   19320 api_server.go:52] waiting for apiserver process to appear ...
I0831 10:57:29.594651   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:29.604532   19320 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0831 10:57:29.604532   19320 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0831 10:57:29.612100   19320 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0831 10:57:29.855601   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
I0831 10:57:29.979727   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 10:57:30.057055   19320 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58422 SSHKeyPath:C:\Users\Venka\.minikube\machines\minikube\id_rsa Username:docker}
W0831 10:57:30.058766   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.094641   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:30.180866   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0831 10:57:30.251231   19320 retry.go:31] will retry after 234.518093ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0831 10:57:30.315504   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.316017   19320 retry.go:31] will retry after 265.906714ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.495828   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:30.568658   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.568658   19320 retry.go:31] will retry after 397.529646ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.591767   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0831 10:57:30.600834   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 10:57:30.668598   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.668598   19320 retry.go:31] will retry after 529.451788ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:30.983808   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:31.046339   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.046339   19320 retry.go:31] will retry after 366.80516ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.109119   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:31.219564   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:31.291923   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.291923   19320 retry.go:31] will retry after 323.76139ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.431370   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:31.497923   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.497923   19320 retry.go:31] will retry after 859.594385ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.611497   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:31.631067   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:31.718307   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:31.718307   19320 retry.go:31] will retry after 1.016631178s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:32.105758   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:32.369678   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:32.437942   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:32.437942   19320 retry.go:31] will retry after 1.63993916s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:32.605824   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:32.747876   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:32.816659   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:32.816659   19320 retry.go:31] will retry after 1.252694624s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:33.095824   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:33.603753   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:34.090889   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 10:57:34.097719   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0831 10:57:34.130769   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 10:57:34.177667   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:34.177667   19320 retry.go:31] will retry after 1.051134631s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0831 10:57:34.183082   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:34.183082   19320 retry.go:31] will retry after 2.515880428s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:34.608766   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:35.104899   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:35.241794   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:35.313952   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:35.313952   19320 retry.go:31] will retry after 4.212867344s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:35.610548   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:36.095289   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:36.610647   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:36.721644   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:36.794793   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:36.794793   19320 retry.go:31] will retry after 2.315452081s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:37.108354   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:37.600290   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:38.108988   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:38.595261   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:39.102725   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:39.122716   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:39.191058   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:39.191058   19320 retry.go:31] will retry after 3.153283059s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:39.542533   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 10:57:39.608705   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 10:57:39.611477   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:39.611477   19320 retry.go:31] will retry after 2.141938567s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:40.097366   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:40.604570   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:41.132090   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:41.602499   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:41.760718   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:41.833060   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:41.833060   19320 retry.go:31] will retry after 8.950806585s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:42.107177   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:42.363397   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:42.441407   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:42.441407   19320 retry.go:31] will retry after 8.616495645s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:42.599046   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:43.105715   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:43.609759   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:44.102109   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:44.619395   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:45.098872   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:45.605710   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:46.108233   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:46.597580   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:47.101457   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:47.608221   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:48.105063   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:48.606674   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:49.107403   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:49.607007   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:50.100342   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:50.605678   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:50.793015   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:57:50.857984   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:50.857984   19320 retry.go:31] will retry after 11.726966503s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:51.075588   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0831 10:57:51.107772   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0831 10:57:51.149735   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:51.149735   19320 retry.go:31] will retry after 5.317378572s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:51.598344   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:52.101806   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:52.609766   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:53.100439   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:53.607081   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:54.097202   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:54.615956   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:55.108238   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:55.598499   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:56.101118   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:56.481525   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:57:56.553119   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:56.553141   19320 retry.go:31] will retry after 12.252174109s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:57:56.607963   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:57.102932   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:57.595815   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:58.103631   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:58.605300   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:59.108342   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:57:59.595416   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:00.103241   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:00.607675   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:01.094924   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:01.598819   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:02.104321   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:02.601941   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:02.657631   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:58:02.729644   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:02.729644   19320 retry.go:31] will retry after 10.49499687s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:03.107500   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:03.599691   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:04.114245   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:04.596361   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:05.103495   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:05.609874   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:06.101118   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:06.606905   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:07.099566   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:07.603786   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:08.108250   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:08.597064   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:08.816683   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0831 10:58:08.884297   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:08.884297   19320 retry.go:31] will retry after 21.229154857s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:09.100563   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:09.602812   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:10.107868   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:10.597789   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:11.104325   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:11.609952   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:12.101503   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:12.608063   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:13.097732   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:13.240598   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0831 10:58:13.308155   19320 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:13.308155   19320 retry.go:31] will retry after 11.674146844s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0831 10:58:13.604248   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:14.100444   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:14.606903   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:15.103218   19320 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0831 10:58:15.130702   19320 api_server.go:72] duration metric: took 46.8942902s to wait for apiserver process to appear ...
I0831 10:58:15.175184   19320 api_server.go:88] waiting for apiserver healthz status ...
I0831 10:58:15.175184   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:19.586159   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0831 10:58:19.586159   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0831 10:58:19.586159   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:19.595127   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0831 10:58:19.595127   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0831 10:58:19.677458   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:21.738999   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:21.738999   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:21.738999   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:21.745950   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:21.745950   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:22.179021   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:22.590998   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:22.590998   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:22.686115   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:22.693311   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:22.693311   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:23.175337   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:23.182231   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:23.182595   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:23.682534   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:23.828589   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:23.828589   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:24.188612   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:24.195343   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:24.195343   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:24.678941   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:24.692356   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:24.692356   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:25.003254   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0831 10:58:25.184847   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:25.192184   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:25.192184   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:25.689717   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:25.697079   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:25.697079   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:26.177688   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:26.184477   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:26.184477   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:26.684930   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:26.822329   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:26.822329   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:27.190964   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:27.197523   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:27.197523   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:27.682010   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:27.688489   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:27.688489   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:28.101524   19320 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.0982691s)
I0831 10:58:28.185055   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:28.201810   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:28.201862   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:28.734292   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0831 10:58:28.755438   19320 logs.go:276] 2 containers: [351f98c58139 109fbebb4967]
I0831 10:58:28.761350   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0831 10:58:28.784606   19320 logs.go:276] 2 containers: [20e3951a13c2 38e3c40e7219]
I0831 10:58:28.790063   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0831 10:58:28.814616   19320 logs.go:276] 2 containers: [51fcd31d4346 238bd222e2d7]
I0831 10:58:28.820546   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0831 10:58:28.842300   19320 logs.go:276] 2 containers: [6f35480baee2 54fd5882840e]
I0831 10:58:28.848150   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0831 10:58:28.869564   19320 logs.go:276] 2 containers: [7e49007da47e 1fcc1c1d2522]
I0831 10:58:28.875474   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0831 10:58:28.897143   19320 logs.go:276] 2 containers: [48d8c776c8d5 8be9b3840129]
I0831 10:58:28.903038   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0831 10:58:28.926981   19320 logs.go:276] 0 containers: []
W0831 10:58:28.926981   19320 logs.go:278] No container was found matching "kindnet"
I0831 10:58:28.933473   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0831 10:58:28.954529   19320 logs.go:276] 1 containers: [e7e299b83b52]
I0831 10:58:29.197575   19320 logs.go:123] Gathering logs for kube-proxy [1fcc1c1d2522] ...
I0831 10:58:29.197575   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1fcc1c1d2522"
I0831 10:58:29.224102   19320 logs.go:123] Gathering logs for kube-controller-manager [8be9b3840129] ...
I0831 10:58:29.341313   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8be9b3840129"
I0831 10:58:29.406047   19320 logs.go:123] Gathering logs for describe nodes ...
I0831 10:58:29.406047   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0831 10:58:30.128769   19320 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0831 10:58:32.056412   19320 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (2.6503651s)
I0831 10:58:32.056412   19320 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.9276432s)
I0831 10:58:32.059209   19320 logs.go:123] Gathering logs for kube-apiserver [351f98c58139] ...
I0831 10:58:32.059209   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 351f98c58139"
I0831 10:58:32.269127   19320 logs.go:123] Gathering logs for kube-proxy [7e49007da47e] ...
I0831 10:58:32.269127   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7e49007da47e"
I0831 10:58:32.319543   19320 logs.go:123] Gathering logs for coredns [238bd222e2d7] ...
I0831 10:58:32.319543   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 238bd222e2d7"
I0831 10:58:32.354883   19320 logs.go:123] Gathering logs for storage-provisioner [e7e299b83b52] ...
I0831 10:58:32.354883   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e7e299b83b52"
I0831 10:58:32.397463   19320 logs.go:123] Gathering logs for kube-apiserver [109fbebb4967] ...
I0831 10:58:32.397463   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 109fbebb4967"
I0831 10:58:32.561954   19320 logs.go:123] Gathering logs for etcd [38e3c40e7219] ...
I0831 10:58:32.561954   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 38e3c40e7219"
I0831 10:58:32.766901   19320 logs.go:123] Gathering logs for coredns [51fcd31d4346] ...
I0831 10:58:32.766901   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 51fcd31d4346"
I0831 10:58:32.797959   19320 logs.go:123] Gathering logs for container status ...
I0831 10:58:32.797959   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0831 10:58:32.839926   19320 logs.go:123] Gathering logs for kubelet ...
I0831 10:58:32.839926   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0831 10:58:32.935181   19320 logs.go:123] Gathering logs for kube-scheduler [6f35480baee2] ...
I0831 10:58:32.935181   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6f35480baee2"
I0831 10:58:33.188702   19320 logs.go:123] Gathering logs for kube-scheduler [54fd5882840e] ...
I0831 10:58:33.188702   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 54fd5882840e"
I0831 10:58:33.231428   19320 logs.go:123] Gathering logs for Docker ...
I0831 10:58:33.231428   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0831 10:58:33.270523   19320 logs.go:123] Gathering logs for dmesg ...
I0831 10:58:33.270523   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0831 10:58:33.291616   19320 logs.go:123] Gathering logs for etcd [20e3951a13c2] ...
I0831 10:58:33.291616   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20e3951a13c2"
I0831 10:58:33.672521   19320 logs.go:123] Gathering logs for kube-controller-manager [48d8c776c8d5] ...
I0831 10:58:33.672521   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 48d8c776c8d5"
I0831 10:58:36.211268   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:36.520671   19320 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0831 10:58:36.445063   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:36.520671   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:36.532559   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0831 10:58:36.649694   19320 addons.go:505] duration metric: took 1m8.650903s for enable addons: enabled=[storage-provisioner default-storageclass]
I0831 10:58:36.671098   19320 logs.go:276] 2 containers: [351f98c58139 109fbebb4967]
I0831 10:58:36.677045   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0831 10:58:36.700932   19320 logs.go:276] 2 containers: [20e3951a13c2 38e3c40e7219]
I0831 10:58:36.706978   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0831 10:58:36.731568   19320 logs.go:276] 2 containers: [51fcd31d4346 238bd222e2d7]
I0831 10:58:36.736965   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0831 10:58:36.757372   19320 logs.go:276] 2 containers: [6f35480baee2 54fd5882840e]
I0831 10:58:36.762727   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0831 10:58:36.783632   19320 logs.go:276] 2 containers: [7e49007da47e 1fcc1c1d2522]
I0831 10:58:36.789019   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0831 10:58:36.814071   19320 logs.go:276] 1 containers: [48d8c776c8d5]
I0831 10:58:36.820031   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0831 10:58:36.840408   19320 logs.go:276] 0 containers: []
W0831 10:58:36.840408   19320 logs.go:278] No container was found matching "kindnet"
I0831 10:58:36.846257   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0831 10:58:36.866938   19320 logs.go:276] 1 containers: [e7e299b83b52]
I0831 10:58:36.866938   19320 logs.go:123] Gathering logs for kube-proxy [1fcc1c1d2522] ...
I0831 10:58:36.866938   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1fcc1c1d2522"
I0831 10:58:36.889656   19320 logs.go:123] Gathering logs for storage-provisioner [e7e299b83b52] ...
I0831 10:58:36.889656   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e7e299b83b52"
I0831 10:58:36.914737   19320 logs.go:123] Gathering logs for container status ...
I0831 10:58:36.914737   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0831 10:58:36.968599   19320 logs.go:123] Gathering logs for dmesg ...
I0831 10:58:36.968599   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0831 10:58:36.989866   19320 logs.go:123] Gathering logs for describe nodes ...
I0831 10:58:36.989866   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0831 10:58:37.607558   19320 logs.go:123] Gathering logs for kube-apiserver [109fbebb4967] ...
I0831 10:58:37.607558   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 109fbebb4967"
I0831 10:58:37.859938   19320 logs.go:123] Gathering logs for kube-proxy [7e49007da47e] ...
I0831 10:58:37.859938   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7e49007da47e"
I0831 10:58:37.883710   19320 logs.go:123] Gathering logs for coredns [238bd222e2d7] ...
I0831 10:58:37.883710   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 238bd222e2d7"
I0831 10:58:37.907566   19320 logs.go:123] Gathering logs for kube-controller-manager [48d8c776c8d5] ...
I0831 10:58:37.907566   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 48d8c776c8d5"
I0831 10:58:37.931409   19320 logs.go:123] Gathering logs for kubelet ...
I0831 10:58:37.931409   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0831 10:58:38.222443   19320 logs.go:123] Gathering logs for etcd [20e3951a13c2] ...
I0831 10:58:38.222443   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20e3951a13c2"
I0831 10:58:38.541188   19320 logs.go:123] Gathering logs for etcd [38e3c40e7219] ...
I0831 10:58:38.541188   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 38e3c40e7219"
I0831 10:58:38.743208   19320 logs.go:123] Gathering logs for coredns [51fcd31d4346] ...
I0831 10:58:38.743208   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 51fcd31d4346"
I0831 10:58:38.771053   19320 logs.go:123] Gathering logs for kube-scheduler [6f35480baee2] ...
I0831 10:58:38.771053   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6f35480baee2"
I0831 10:58:38.799731   19320 logs.go:123] Gathering logs for kube-apiserver [351f98c58139] ...
I0831 10:58:38.799731   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 351f98c58139"
I0831 10:58:38.843918   19320 logs.go:123] Gathering logs for kube-scheduler [54fd5882840e] ...
I0831 10:58:38.843918   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 54fd5882840e"
I0831 10:58:38.885698   19320 logs.go:123] Gathering logs for Docker ...
I0831 10:58:38.886204   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0831 10:58:41.430580   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:41.438378   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0831 10:58:41.438378   19320 api_server.go:103] status: https://127.0.0.1:58426/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0831 10:58:41.444180   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0831 10:58:41.465985   19320 logs.go:276] 2 containers: [351f98c58139 109fbebb4967]
I0831 10:58:41.471911   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0831 10:58:41.492257   19320 logs.go:276] 2 containers: [20e3951a13c2 38e3c40e7219]
I0831 10:58:41.499777   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0831 10:58:41.523511   19320 logs.go:276] 2 containers: [51fcd31d4346 238bd222e2d7]
I0831 10:58:41.529391   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0831 10:58:41.551548   19320 logs.go:276] 2 containers: [6f35480baee2 54fd5882840e]
I0831 10:58:41.557940   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0831 10:58:41.580188   19320 logs.go:276] 2 containers: [7e49007da47e 1fcc1c1d2522]
I0831 10:58:41.586632   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0831 10:58:41.606889   19320 logs.go:276] 1 containers: [48d8c776c8d5]
I0831 10:58:41.613250   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0831 10:58:41.634026   19320 logs.go:276] 0 containers: []
W0831 10:58:41.634026   19320 logs.go:278] No container was found matching "kindnet"
I0831 10:58:41.639327   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0831 10:58:41.661307   19320 logs.go:276] 1 containers: [e7e299b83b52]
I0831 10:58:41.661307   19320 logs.go:123] Gathering logs for coredns [51fcd31d4346] ...
I0831 10:58:41.661307   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 51fcd31d4346"
I0831 10:58:41.688988   19320 logs.go:123] Gathering logs for kube-scheduler [54fd5882840e] ...
I0831 10:58:41.688988   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 54fd5882840e"
I0831 10:58:41.738968   19320 logs.go:123] Gathering logs for kube-proxy [7e49007da47e] ...
I0831 10:58:41.738968   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7e49007da47e"
I0831 10:58:41.764377   19320 logs.go:123] Gathering logs for storage-provisioner [e7e299b83b52] ...
I0831 10:58:41.764377   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e7e299b83b52"
I0831 10:58:41.787705   19320 logs.go:123] Gathering logs for dmesg ...
I0831 10:58:41.787705   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0831 10:58:41.808845   19320 logs.go:123] Gathering logs for kube-proxy [1fcc1c1d2522] ...
I0831 10:58:41.808845   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1fcc1c1d2522"
I0831 10:58:41.832494   19320 logs.go:123] Gathering logs for Docker ...
I0831 10:58:41.832999   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0831 10:58:41.869272   19320 logs.go:123] Gathering logs for container status ...
I0831 10:58:41.869272   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0831 10:58:41.916090   19320 logs.go:123] Gathering logs for kubelet ...
I0831 10:58:41.916090   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0831 10:58:42.017173   19320 logs.go:123] Gathering logs for describe nodes ...
I0831 10:58:42.017173   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0831 10:58:44.504818   19320 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (2.4876449s)
I0831 10:58:44.612084   19320 logs.go:123] Gathering logs for etcd [38e3c40e7219] ...
I0831 10:58:44.612165   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 38e3c40e7219"
I0831 10:58:44.758744   19320 logs.go:123] Gathering logs for coredns [238bd222e2d7] ...
I0831 10:58:44.758744   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 238bd222e2d7"
I0831 10:58:44.783456   19320 logs.go:123] Gathering logs for kube-controller-manager [48d8c776c8d5] ...
I0831 10:58:44.783456   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 48d8c776c8d5"
I0831 10:58:44.808413   19320 logs.go:123] Gathering logs for kube-apiserver [351f98c58139] ...
I0831 10:58:44.808413   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 351f98c58139"
I0831 10:58:44.862091   19320 logs.go:123] Gathering logs for kube-apiserver [109fbebb4967] ...
I0831 10:58:44.862091   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 109fbebb4967"
I0831 10:58:44.940507   19320 logs.go:123] Gathering logs for etcd [20e3951a13c2] ...
I0831 10:58:44.940507   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20e3951a13c2"
I0831 10:58:45.073619   19320 logs.go:123] Gathering logs for kube-scheduler [6f35480baee2] ...
I0831 10:58:45.073619   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6f35480baee2"
I0831 10:58:47.612883   19320 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58426/healthz ...
I0831 10:58:48.953902   19320 api_server.go:279] https://127.0.0.1:58426/healthz returned 200:
ok
I0831 10:58:49.201835   19320 api_server.go:141] control plane version: v1.30.0
I0831 10:58:49.835819   19320 api_server.go:131] duration metric: took 34.6606345s to wait for apiserver health ...
I0831 10:58:50.237691   19320 system_pods.go:43] waiting for kube-system pods to appear ...
I0831 10:58:50.242693   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0831 10:58:50.264935   19320 logs.go:276] 2 containers: [351f98c58139 109fbebb4967]
I0831 10:58:50.271937   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0831 10:58:50.295259   19320 logs.go:276] 2 containers: [20e3951a13c2 38e3c40e7219]
I0831 10:58:50.300644   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0831 10:58:50.321454   19320 logs.go:276] 2 containers: [51fcd31d4346 238bd222e2d7]
I0831 10:58:50.327360   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0831 10:58:50.346318   19320 logs.go:276] 2 containers: [6f35480baee2 54fd5882840e]
I0831 10:58:50.351693   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0831 10:58:50.376810   19320 logs.go:276] 2 containers: [7e49007da47e 1fcc1c1d2522]
I0831 10:58:50.384466   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0831 10:58:50.404106   19320 logs.go:276] 1 containers: [48d8c776c8d5]
I0831 10:58:50.410017   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0831 10:58:50.429232   19320 logs.go:276] 0 containers: []
W0831 10:58:50.429232   19320 logs.go:278] No container was found matching "kindnet"
I0831 10:58:50.436162   19320 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0831 10:58:50.457584   19320 logs.go:276] 2 containers: [485441f1d798 e7e299b83b52]
I0831 10:58:50.457584   19320 logs.go:123] Gathering logs for dmesg ...
I0831 10:58:50.457584   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0831 10:58:50.478293   19320 logs.go:123] Gathering logs for kube-scheduler [6f35480baee2] ...
I0831 10:58:50.478293   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6f35480baee2"
I0831 10:58:50.508312   19320 logs.go:123] Gathering logs for storage-provisioner [e7e299b83b52] ...
I0831 10:58:50.508312   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e7e299b83b52"
I0831 10:58:50.532191   19320 logs.go:123] Gathering logs for container status ...
I0831 10:58:50.532191   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0831 10:58:50.576812   19320 logs.go:123] Gathering logs for kube-apiserver [351f98c58139] ...
I0831 10:58:50.576812   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 351f98c58139"
I0831 10:58:50.627086   19320 logs.go:123] Gathering logs for etcd [38e3c40e7219] ...
I0831 10:58:50.627130   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 38e3c40e7219"
I0831 10:58:50.733813   19320 logs.go:123] Gathering logs for kube-proxy [1fcc1c1d2522] ...
I0831 10:58:50.733813   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1fcc1c1d2522"
I0831 10:58:50.759847   19320 logs.go:123] Gathering logs for kube-controller-manager [48d8c776c8d5] ...
I0831 10:58:50.759847   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 48d8c776c8d5"
I0831 10:58:50.786116   19320 logs.go:123] Gathering logs for Docker ...
I0831 10:58:50.786116   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0831 10:58:50.826906   19320 logs.go:123] Gathering logs for describe nodes ...
I0831 10:58:50.826906   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0831 10:59:00.449442   19320 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (9.6225354s)
I0831 10:59:00.452182   19320 logs.go:123] Gathering logs for kube-apiserver [109fbebb4967] ...
I0831 10:59:00.452182   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 109fbebb4967"
I0831 10:59:00.525308   19320 logs.go:123] Gathering logs for coredns [51fcd31d4346] ...
I0831 10:59:00.525308   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 51fcd31d4346"
I0831 10:59:00.553176   19320 logs.go:123] Gathering logs for kube-scheduler [54fd5882840e] ...
I0831 10:59:00.553176   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 54fd5882840e"
I0831 10:59:01.119002   19320 logs.go:123] Gathering logs for storage-provisioner [485441f1d798] ...
I0831 10:59:01.119002   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 485441f1d798"
I0831 10:59:01.141193   19320 logs.go:123] Gathering logs for kubelet ...
I0831 10:59:01.141193   19320 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0831 10:59:01.246673   19320 logs.go:123] Gathering logs for etcd [20e3951a13c2] ...
I0831 10:59:01.246673   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20e3951a13c2"
I0831 10:59:01.533656   19320 logs.go:123] Gathering logs for coredns [238bd222e2d7] ...
I0831 10:59:01.533656   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 238bd222e2d7"
I0831 10:59:01.560743   19320 logs.go:123] Gathering logs for kube-proxy [7e49007da47e] ...
I0831 10:59:01.560743   19320 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7e49007da47e"
I0831 10:59:10.065471   19320 system_pods.go:59] 7 kube-system pods found
I0831 10:59:10.065471   19320 system_pods.go:61] "coredns-7db6d8ff4d-7lvhw" [40bd65aa-2d53-4d17-9153-990b43b38690] Running
I0831 10:59:10.065471   19320 system_pods.go:61] "etcd-minikube" [148c86ba-6071-4a42-9bfd-6eb4f554d23f] Running
I0831 10:59:10.065471   19320 system_pods.go:61] "kube-apiserver-minikube" [574160b5-997b-41d9-b6b1-34f2fcf970dd] Running
I0831 10:59:10.065471   19320 system_pods.go:61] "kube-controller-manager-minikube" [b7b2f011-bb69-4b33-95c7-a01410ce4958] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0831 10:59:10.065471   19320 system_pods.go:61] "kube-proxy-t44zv" [bf827197-1c3a-49dd-b49b-1f11c2cb32c4] Running
I0831 10:59:10.065471   19320 system_pods.go:61] "kube-scheduler-minikube" [f4616374-4b36-478d-aed8-759a9d03b307] Running
I0831 10:59:10.065471   19320 system_pods.go:61] "storage-provisioner" [36fc01fb-7aee-456b-aef9-8df527ca5d30] Running
I0831 10:59:10.065471   19320 system_pods.go:74] duration metric: took 19.8277797s to wait for pod list to return data ...
I0831 10:59:10.065471   19320 kubeadm.go:576] duration metric: took 1m41.8290598s to wait for: map[apiserver:true system_pods:true]
I0831 10:59:10.065471   19320 node_conditions.go:102] verifying NodePressure condition ...
I0831 10:59:18.156680   19320 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0831 10:59:18.156680   19320 node_conditions.go:123] node cpu capacity is 8
I0831 10:59:18.231525   19320 node_conditions.go:105] duration metric: took 8.1660534s to run NodePressure ...
I0831 10:59:18.231525   19320 start.go:240] waiting for startup goroutines ...
I0831 10:59:18.231525   19320 start.go:245] waiting for cluster config update ...
I0831 10:59:18.231525   19320 start.go:254] writing updated cluster config ...
I0831 10:59:18.239529   19320 ssh_runner.go:195] Run: rm -f paused
I0831 10:59:23.948006   19320 start.go:600] kubectl: 1.31.0, cluster: 1.30.0 (minor skew: 1)
I0831 10:59:24.111896   19320 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 31 19:27:12 minikube dockerd[7449]: time="2024-08-31T19:27:12.451819706Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 19:32:39 minikube dockerd[7449]: time="2024-08-31T19:32:39.583544041Z" level=info msg="ignoring event" container=69b0f82a5e7729488da3880e706237e845a28217604004b5c389f27c095d9864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 31 19:32:41 minikube dockerd[7449]: time="2024-08-31T19:32:41.032346739Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.254:53: server misbehaving" spanID=48fec7e89c63335b traceID=f9ac1079e4118789775b5585bea3dd92
Aug 31 19:32:41 minikube dockerd[7449]: time="2024-08-31T19:32:41.032429039Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.254:53: server misbehaving" spanID=48fec7e89c63335b traceID=f9ac1079e4118789775b5585bea3dd92
Aug 31 19:32:54 minikube dockerd[7449]: time="2024-08-31T19:32:54.599473663Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.254:53: server misbehaving" spanID=48fec7e89c63335b traceID=f9ac1079e4118789775b5585bea3dd92
Aug 31 19:38:08 minikube dockerd[7449]: time="2024-08-31T19:38:08.053313654Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3a04ca564164ca7d traceID=7fa05403e5ebc359bd904f8fc134096b
Aug 31 19:38:08 minikube dockerd[7449]: time="2024-08-31T19:38:08.053381054Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 19:41:47 minikube cri-dockerd[7848]: time="2024-08-31T19:41:47Z" level=error msg="Exec session in the container terminated but process still running! Session de746bab7f79f47fa4f4d7cb2a941398738ae17a0daa5f6267f3b81f7ae8ae0e | Container 0a48497e3181f2dc193ddccddff28b46644b33386c330214dcf12afe7839512c"
Aug 31 19:43:10 minikube cri-dockerd[7848]: time="2024-08-31T19:43:10Z" level=error msg="Exec session in the container terminated but process still running! Session a4637fcbece298d5f1ea529ac27129bd2ad13f4b3238ff8592a1cce030441739 | Container 0a48497e3181f2dc193ddccddff28b46644b33386c330214dcf12afe7839512c"
Aug 31 19:43:19 minikube dockerd[7449]: time="2024-08-31T19:43:19.106711727Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=efc6d39039c41037 traceID=b1d866b3438c0ec546d25b59d3502413
Aug 31 19:43:19 minikube dockerd[7449]: time="2024-08-31T19:43:19.106804527Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 19:48:25 minikube dockerd[7449]: time="2024-08-31T19:48:25.419097326Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=bed5af8c5313b698 traceID=d27fac2e6a44aa477d9d05fda32505ef
Aug 31 19:48:25 minikube dockerd[7449]: time="2024-08-31T19:48:25.419199428Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 19:53:28 minikube dockerd[7449]: time="2024-08-31T19:53:28.067611776Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=67d462733372b605 traceID=4e25090fda741c154759e38b7cbab09e
Aug 31 19:53:28 minikube dockerd[7449]: time="2024-08-31T19:53:28.067705076Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 19:58:36 minikube dockerd[7449]: time="2024-08-31T19:58:36.430729460Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b6820681cb78a98d traceID=9008165157f14584efdc217650edd1fb
Aug 31 19:58:36 minikube dockerd[7449]: time="2024-08-31T19:58:36.430822779Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:03:38 minikube dockerd[7449]: time="2024-08-31T20:03:38.425037351Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=00b33dc805448bc0 traceID=6f7a8b5e58f8a2ae00fefd656c89e2a4
Aug 31 20:03:38 minikube dockerd[7449]: time="2024-08-31T20:03:38.425129651Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:08:42 minikube dockerd[7449]: time="2024-08-31T20:08:42.604897396Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=6a7c4929b591ada1 traceID=06384494d78d0aa59e2e1844755645fb
Aug 31 20:08:42 minikube dockerd[7449]: time="2024-08-31T20:08:42.604979696Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:13:54 minikube dockerd[7449]: time="2024-08-31T20:13:54.485491026Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=a665ab3315e91b68 traceID=03105bf87dd035fe45d98180eedec573
Aug 31 20:13:54 minikube dockerd[7449]: time="2024-08-31T20:13:54.485586926Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:19:06 minikube dockerd[7449]: time="2024-08-31T20:19:06.808409216Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b9f04647aa35f38b traceID=b4f71401bbedfddff2cf5cf95a841bec
Aug 31 20:19:06 minikube dockerd[7449]: time="2024-08-31T20:19:06.808483016Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:24:16 minikube dockerd[7449]: time="2024-08-31T20:24:16.197329927Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=2b0ef5499fd6fa8c traceID=45761272eeeee7644cac56ff13e6092e
Aug 31 20:24:16 minikube dockerd[7449]: time="2024-08-31T20:24:16.197444928Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:29:22 minikube dockerd[7449]: time="2024-08-31T20:29:22.409305348Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=a244ec407a4cd8fc traceID=d820e469b4f6d633ca15efc35a53b25c
Aug 31 20:29:22 minikube dockerd[7449]: time="2024-08-31T20:29:22.409398248Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:34:34 minikube dockerd[7449]: time="2024-08-31T20:34:34.791917782Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3522cbcd591aee6c traceID=fcca5101d071477ea217db7d8c2d227c
Aug 31 20:34:34 minikube dockerd[7449]: time="2024-08-31T20:34:34.792006582Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:39:38 minikube dockerd[7449]: time="2024-08-31T20:39:38.656240068Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e0a2faa37f820bd9 traceID=8a0e77f2e2ddde987727653d091e7fee
Aug 31 20:39:38 minikube dockerd[7449]: time="2024-08-31T20:39:38.656300968Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:44:47 minikube dockerd[7449]: time="2024-08-31T20:44:47.801724370Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b6b6124b281eab8f traceID=db6c129ca269f57200f27d0dbd1a9752
Aug 31 20:44:47 minikube dockerd[7449]: time="2024-08-31T20:44:47.801808171Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:48:23 minikube cri-dockerd[7848]: time="2024-08-31T20:48:23Z" level=error msg="Exec session in the container terminated but process still running! Session b018058de2665e33e37d48c39db16dde27fcf0d9ac674c99f3743b479cff0106 | Container 0a48497e3181f2dc193ddccddff28b46644b33386c330214dcf12afe7839512c"
Aug 31 20:49:52 minikube dockerd[7449]: time="2024-08-31T20:49:52.632745064Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=8daf430c0973f21e traceID=dc289662521568d937689cef96ebebb8
Aug 31 20:49:52 minikube dockerd[7449]: time="2024-08-31T20:49:52.632838163Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:54:54 minikube dockerd[7449]: time="2024-08-31T20:54:54.240938284Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=906488a56d82448b traceID=660783e329319fbd0664457cfb1ee456
Aug 31 20:54:54 minikube dockerd[7449]: time="2024-08-31T20:54:54.241037585Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 20:59:59 minikube dockerd[7449]: time="2024-08-31T20:59:59.870223293Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e801afcfdee450f2 traceID=908b306699512402d77afa32cd12bbb4
Aug 31 20:59:59 minikube dockerd[7449]: time="2024-08-31T20:59:59.870314794Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:05:09 minikube dockerd[7449]: time="2024-08-31T21:05:09.564005232Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ad96edcc1a3d822c traceID=c3cee92d981da7b0a946eb2b88c594c1
Aug 31 21:05:09 minikube dockerd[7449]: time="2024-08-31T21:05:09.564080032Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:10:14 minikube dockerd[7449]: time="2024-08-31T21:10:14.553009827Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=d04657b3035cca4f traceID=f4a6fb0fa717b59b1662ba8761e013af
Aug 31 21:10:14 minikube dockerd[7449]: time="2024-08-31T21:10:14.553103627Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:15:21 minikube dockerd[7449]: time="2024-08-31T21:15:21.616761895Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=21b97f648ab8b58c traceID=befea34e5dc5aedd06466116b112237c
Aug 31 21:15:21 minikube dockerd[7449]: time="2024-08-31T21:15:21.616829695Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:20:30 minikube dockerd[7449]: time="2024-08-31T21:20:30.564592207Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=060f5e2643ecba69 traceID=c8a4e9bd6fa63eb0a281fedd67562e52
Aug 31 21:20:30 minikube dockerd[7449]: time="2024-08-31T21:20:30.564692607Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:25:37 minikube dockerd[7449]: time="2024-08-31T21:25:37.606702793Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=1792f930eeb26fe9 traceID=5c40a4f187838fba2c1a5d1bdc1f3c32
Aug 31 21:25:37 minikube dockerd[7449]: time="2024-08-31T21:25:37.606950496Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:30:46 minikube dockerd[7449]: time="2024-08-31T21:30:46.451625879Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=04af5135fd33196a traceID=078bc146a8b69a543be064c44494e5c7
Aug 31 21:30:46 minikube dockerd[7449]: time="2024-08-31T21:30:46.451892778Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:35:50 minikube dockerd[7449]: time="2024-08-31T21:35:50.001439347Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=4ac290375a97173d traceID=a16923684166aa2af4ae1d4e671eee33
Aug 31 21:35:50 minikube dockerd[7449]: time="2024-08-31T21:35:50.001513847Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:41:01 minikube dockerd[7449]: time="2024-08-31T21:41:01.580877966Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=6e70bec8fcb143a5 traceID=8d65c3e28cf75ed0e9b657cb7e0b490a
Aug 31 21:41:01 minikube dockerd[7449]: time="2024-08-31T21:41:01.581005266Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 31 21:46:02 minikube dockerd[7449]: time="2024-08-31T21:46:02.426133257Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=af8e467b1686e910 traceID=d710998b630e8b845203c5dc90958904
Aug 31 21:46:02 minikube dockerd[7449]: time="2024-08-31T21:46:02.426230663Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
5fe05e0372418       6e38f40d628db                                                                      2 hours ago         Running             storage-provisioner       12                  945d9442cea3f       storage-provisioner
69b0f82a5e772       6e38f40d628db                                                                      3 hours ago         Exited              storage-provisioner       11                  945d9442cea3f       storage-provisioner
0a48497e3181f       postgres@sha256:c62fdb7fd6f519ef425c54760894c74e8d0cb04fbf4f7d3d79aafd86bae24edd   5 hours ago         Running             postgres                  0                   6c34b715f5fb2       postgres-6fb6f565d6-7cfdx
055341c90545e       c7aad43836fa5                                                                      6 hours ago         Running             kube-controller-manager   3                   44742d01ca117       kube-controller-manager-minikube
7e49007da47ef       a0bf559e280cf                                                                      6 hours ago         Running             kube-proxy                1                   1875f38e80642       kube-proxy-t44zv
351f98c581394       c42f13656d0b2                                                                      6 hours ago         Running             kube-apiserver            1                   264be3a690a61       kube-apiserver-minikube
20e3951a13c21       3861cfcd7c04c                                                                      6 hours ago         Running             etcd                      1                   1e2205d92b7a7       etcd-minikube
6f35480baee27       259c8277fcbbc                                                                      6 hours ago         Running             kube-scheduler            1                   99508a09c65fd       kube-scheduler-minikube
51fcd31d43460       cbb01a7bd410d                                                                      6 hours ago         Running             coredns                   1                   1d3cbf4aa0f40       coredns-7db6d8ff4d-7lvhw
1fcc1c1d2522a       a0bf559e280cf                                                                      10 hours ago        Exited              kube-proxy                0                   4027f97f4d63c       kube-proxy-t44zv
238bd222e2d74       cbb01a7bd410d                                                                      10 hours ago        Exited              coredns                   0                   bb91bfe4053b2       coredns-7db6d8ff4d-7lvhw
109fbebb49679       c42f13656d0b2                                                                      10 hours ago        Exited              kube-apiserver            0                   6dc963621fd15       kube-apiserver-minikube
38e3c40e72190       3861cfcd7c04c                                                                      10 hours ago        Exited              etcd                      0                   2f53d6be4676d       etcd-minikube
54fd5882840e5       259c8277fcbbc                                                                      10 hours ago        Exited              kube-scheduler            0                   a0cf921014d16       kube-scheduler-minikube


==> coredns [238bd222e2d7] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:34584 - 25286 "HINFO IN 8629584568144905830.3455304890382980367. udp 57 false 512" NOERROR qr,rd,ra 57 0.072010124s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[245084850]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (31-Aug-2024 12:03:33.144) (total time: 10003ms):
Trace[245084850]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (12:03:43.147)
Trace[245084850]: [10.003339195s] [10.003339195s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[505723965]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (31-Aug-2024 12:03:33.144) (total time: 10003ms):
Trace[505723965]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (12:03:43.147)
Trace[505723965]: [10.003633897s] [10.003633897s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[940337592]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (31-Aug-2024 12:03:33.144) (total time: 10003ms):
Trace[940337592]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (12:03:43.147)
Trace[940337592]: [10.003721198s] [10.003721198s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [51fcd31d4346] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:41159 - 22631 "HINFO IN 3418064528968964798.4351093422088096329. udp 57 false 512" NOERROR qr,rd,ra 57 0.087836675s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_31T07_02_57_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 31 Aug 2024 12:02:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 31 Aug 2024 21:47:47 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 31 Aug 2024 21:44:48 +0000   Sat, 31 Aug 2024 16:26:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 31 Aug 2024 21:44:48 +0000   Sat, 31 Aug 2024 16:26:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 31 Aug 2024 21:44:48 +0000   Sat, 31 Aug 2024 16:26:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 31 Aug 2024 21:44:48 +0000   Sat, 31 Aug 2024 16:26:17 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8073248Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8073248Ki
  pods:               110
System Info:
  Machine ID:                 45aa24cc4f614149ae2a9c9efc4c3b61
  System UUID:                45aa24cc4f614149ae2a9c9efc4c3b61
  Boot ID:                    9d145945-a89d-4563-8abf-6c1ec84f8b19
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     customer-model-57dd76499c-xjqtt     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h23m
  default                     postgres-6fb6f565d6-7cfdx           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h23m
  kube-system                 coredns-7db6d8ff4d-7lvhw            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     9h
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         9h
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-proxy-t44zv                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[  +0.000593] FS-Cache: O-key=[10] '34323935383439323930'
[  +0.000386] FS-Cache: N-cookie c=0000002d [p=00000002 fl=2 nc=0 na=1]
[  +0.000498] FS-Cache: N-cookie d=0000000006aa67a1{9P.session} n=00000000f7d18fe8
[  +0.000642] FS-Cache: N-key=[10] '34323935383439323930'
[  +0.285985] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.087258] FS-Cache: Duplicate cookie detected
[  +0.000528] FS-Cache: O-cookie c=0000002e [p=00000002 fl=222 nc=0 na=1]
[  +0.000473] FS-Cache: O-cookie d=0000000006aa67a1{9P.session} n=00000000e03d87c3
[  +0.000516] FS-Cache: O-key=[10] '34323935383439333237'
[  +0.000354] FS-Cache: N-cookie c=0000002f [p=00000002 fl=2 nc=0 na=1]
[  +0.000448] FS-Cache: N-cookie d=0000000006aa67a1{9P.session} n=00000000213170f5
[  +0.000523] FS-Cache: N-key=[10] '34323935383439333237'
[  +0.007749] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.827545] FS-Cache: Duplicate cookie detected
[  +0.000554] FS-Cache: O-cookie c=00000033 [p=00000002 fl=222 nc=0 na=1]
[  +0.000379] FS-Cache: O-cookie d=0000000006aa67a1{9P.session} n=00000000a06459c8
[  +0.000452] FS-Cache: O-key=[10] '34323935383439343131'
[  +0.000313] FS-Cache: N-cookie c=00000034 [p=00000002 fl=2 nc=0 na=1]
[  +0.000385] FS-Cache: N-cookie d=0000000006aa67a1{9P.session} n=00000000220856df
[  +0.000539] FS-Cache: N-key=[10] '34323935383439343131'
[  +0.010129] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000773] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000902] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.000798] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.175289] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000769] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001446] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000682] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001439] WSL (5) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000811] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.006752] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +2.146773] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.000117] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003516] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000099] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000449] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000414] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000374] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000439] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000384] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000443] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000408] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.927822] netlink: 'init': attribute type 4 has an invalid length.
[Aug31 10:46] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.000062] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[Aug31 13:10] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.000168] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[Aug31 15:12] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.000032] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[Aug31 16:14] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.000245] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[Aug31 17:39] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?
[  +0.000036] WSL (1) WARNING: /usr/share/zoneinfo/America/Chicago not found. Is the tzdata package installed?


==> etcd [20e3951a13c2] <==
{"level":"warn","ts":"2024-08-31T21:44:50.501083Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.479069ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253723 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:15461 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128031583213253721 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T21:44:50.501259Z","caller":"traceutil/trace.go:171","msg":"trace[1084075852] transaction","detail":"{read_only:false; response_revision:15469; number_of_response:1; }","duration":"144.330246ms","start":"2024-08-31T21:44:50.356902Z","end":"2024-08-31T21:44:50.501232Z","steps":["trace[1084075852] 'process raft request'  (duration: 37.646014ms)","trace[1084075852] 'compare'  (duration: 106.374276ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:44:58.666373Z","caller":"traceutil/trace.go:171","msg":"trace[1540754392] transaction","detail":"{read_only:false; response_revision:15475; number_of_response:1; }","duration":"158.506684ms","start":"2024-08-31T21:44:58.50785Z","end":"2024-08-31T21:44:58.666356Z","steps":["trace[1540754392] 'process raft request'  (duration: 158.376071ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:45:02.99153Z","caller":"traceutil/trace.go:171","msg":"trace[1905673389] transaction","detail":"{read_only:false; response_revision:15479; number_of_response:1; }","duration":"120.454667ms","start":"2024-08-31T21:45:02.87106Z","end":"2024-08-31T21:45:02.991514Z","steps":["trace[1905673389] 'process raft request'  (duration: 120.347938ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:45:05.142817Z","caller":"traceutil/trace.go:171","msg":"trace[1822899014] linearizableReadLoop","detail":"{readStateIndex:19843; appliedIndex:19842; }","duration":"136.571559ms","start":"2024-08-31T21:45:05.006231Z","end":"2024-08-31T21:45:05.142803Z","steps":["trace[1822899014] 'read index received'  (duration: 136.285649ms)","trace[1822899014] 'applied index is now lower than readState.Index'  (duration: 285.311s)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:45:05.142903Z","caller":"traceutil/trace.go:171","msg":"trace[206351443] transaction","detail":"{read_only:false; response_revision:15481; number_of_response:1; }","duration":"145.382604ms","start":"2024-08-31T21:45:04.997505Z","end":"2024-08-31T21:45:05.142887Z","steps":["trace[206351443] 'process raft request'  (duration: 145.182437ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:05.142919Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.69018ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T21:45:05.142951Z","caller":"traceutil/trace.go:171","msg":"trace[1892697420] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:15481; }","duration":"136.755836ms","start":"2024-08-31T21:45:05.006186Z","end":"2024-08-31T21:45:05.142942Z","steps":["trace[1892697420] 'agreement among raft nodes before linearized reading'  (duration: 136.688081ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:05.693619Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.238848ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253785 > lease_revoke:<id:70cc91a92894f459>","response":"size:29"}
{"level":"warn","ts":"2024-08-31T21:45:15.770091Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"176.279607ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253829 > lease_revoke:<id:70cc91a92894f481>","response":"size:29"}
{"level":"warn","ts":"2024-08-31T21:45:19.24033Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"239.282787ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253840 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:15257 > ","response":"size:5"}
{"level":"info","ts":"2024-08-31T21:45:19.240404Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15257}
{"level":"info","ts":"2024-08-31T21:45:19.240437Z","caller":"traceutil/trace.go:171","msg":"trace[2108930487] compact","detail":"{revision:15257; response_revision:15492; }","duration":"353.734455ms","start":"2024-08-31T21:45:18.886686Z","end":"2024-08-31T21:45:19.24042Z","steps":["trace[2108930487] 'process raft request'  (duration: 114.310978ms)","trace[2108930487] 'check and update compact revision'  (duration: 239.195033ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T21:45:19.240476Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T21:45:18.886657Z","time spent":"353.816706ms","remote":"127.0.0.1:39040","response type":"/etcdserverpb.KV/Compact","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-08-31T21:45:19.372259Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":15257,"took":"131.638619ms","hash":771258093,"current-db-size-bytes":2371584,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-08-31T21:45:19.372334Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":771258093,"revision":15257,"compact-revision":15036}
{"level":"info","ts":"2024-08-31T21:45:19.79421Z","caller":"traceutil/trace.go:171","msg":"trace[1524777753] transaction","detail":"{read_only:false; response_revision:15493; number_of_response:1; }","duration":"176.842262ms","start":"2024-08-31T21:45:19.617351Z","end":"2024-08-31T21:45:19.794193Z","steps":["trace[1524777753] 'process raft request'  (duration: 176.748402ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:20.147457Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.655481ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T21:45:20.147555Z","caller":"traceutil/trace.go:171","msg":"trace[2096486581] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:15493; }","duration":"139.789666ms","start":"2024-08-31T21:45:20.007744Z","end":"2024-08-31T21:45:20.147533Z","steps":["trace[2096486581] 'range keys from in-memory index tree'  (duration: 139.553717ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:20.594727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"181.663206ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253849 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:15485 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128031583213253847 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T21:45:20.594843Z","caller":"traceutil/trace.go:171","msg":"trace[504949829] transaction","detail":"{read_only:false; response_revision:15494; number_of_response:1; }","duration":"263.672188ms","start":"2024-08-31T21:45:20.331152Z","end":"2024-08-31T21:45:20.594824Z","steps":["trace[504949829] 'process raft request'  (duration: 81.849782ms)","trace[504949829] 'compare'  (duration: 181.57585ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:45:30.095043Z","caller":"traceutil/trace.go:171","msg":"trace[1402945649] transaction","detail":"{read_only:false; response_revision:15501; number_of_response:1; }","duration":"100.476063ms","start":"2024-08-31T21:45:29.994541Z","end":"2024-08-31T21:45:30.095017Z","steps":["trace[1402945649] 'process raft request'  (duration: 100.320637ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:45:38.966689Z","caller":"traceutil/trace.go:171","msg":"trace[1368529799] transaction","detail":"{read_only:false; response_revision:15508; number_of_response:1; }","duration":"122.733055ms","start":"2024-08-31T21:45:38.843931Z","end":"2024-08-31T21:45:38.966664Z","steps":["trace[1368529799] 'process raft request'  (duration: 64.844382ms)","trace[1368529799] 'compare'  (duration: 57.817504ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:45:50.479963Z","caller":"traceutil/trace.go:171","msg":"trace[1417134487] transaction","detail":"{read_only:false; response_revision:15518; number_of_response:1; }","duration":"163.080734ms","start":"2024-08-31T21:45:50.316856Z","end":"2024-08-31T21:45:50.479936Z","steps":["trace[1417134487] 'process raft request'  (duration: 64.221153ms)","trace[1417134487] 'compare'  (duration: 98.748481ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:45:53.036112Z","caller":"traceutil/trace.go:171","msg":"trace[1425723777] transaction","detail":"{read_only:false; response_revision:15520; number_of_response:1; }","duration":"186.257753ms","start":"2024-08-31T21:45:52.849832Z","end":"2024-08-31T21:45:53.03609Z","steps":["trace[1425723777] 'process raft request'  (duration: 186.128853ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:53.408212Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"241.538898ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-08-31T21:45:53.408296Z","caller":"traceutil/trace.go:171","msg":"trace[1376352315] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:15520; }","duration":"241.654798ms","start":"2024-08-31T21:45:53.166625Z","end":"2024-08-31T21:45:53.40828Z","steps":["trace[1376352315] 'count revisions from in-memory index tree'  (duration: 241.417698ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:45:55.270799Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.407173ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213253980 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:15520 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T21:45:55.270895Z","caller":"traceutil/trace.go:171","msg":"trace[1172843224] transaction","detail":"{read_only:false; response_revision:15522; number_of_response:1; }","duration":"210.29924ms","start":"2024-08-31T21:45:55.060574Z","end":"2024-08-31T21:45:55.270874Z","steps":["trace[1172843224] 'process raft request'  (duration: 69.743757ms)","trace[1172843224] 'compare'  (duration: 140.268565ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:46:00.093381Z","caller":"traceutil/trace.go:171","msg":"trace[705311523] transaction","detail":"{read_only:false; response_revision:15525; number_of_response:1; }","duration":"205.064705ms","start":"2024-08-31T21:45:59.888299Z","end":"2024-08-31T21:46:00.093364Z","steps":["trace[705311523] 'process raft request'  (duration: 204.869793ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:46:00.481282Z","caller":"traceutil/trace.go:171","msg":"trace[1208662218] transaction","detail":"{read_only:false; response_revision:15526; number_of_response:1; }","duration":"113.021023ms","start":"2024-08-31T21:46:00.368243Z","end":"2024-08-31T21:46:00.481264Z","steps":["trace[1208662218] 'process raft request'  (duration: 65.190066ms)","trace[1208662218] 'compare'  (duration: 47.723251ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:46:06.006854Z","caller":"traceutil/trace.go:171","msg":"trace[621059128] linearizableReadLoop","detail":"{readStateIndex:19906; appliedIndex:19905; }","duration":"409.177648ms","start":"2024-08-31T21:46:05.597646Z","end":"2024-08-31T21:46:06.006824Z","steps":["trace[621059128] 'read index received'  (duration: 408.71233ms)","trace[621059128] 'applied index is now lower than readState.Index'  (duration: 464.018s)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:46:06.007068Z","caller":"traceutil/trace.go:171","msg":"trace[26648868] transaction","detail":"{read_only:false; response_revision:15529; number_of_response:1; }","duration":"580.097385ms","start":"2024-08-31T21:46:05.426924Z","end":"2024-08-31T21:46:06.007021Z","steps":["trace[26648868] 'process raft request'  (duration: 579.496762ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:46:06.007123Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"409.445158ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1110"}
{"level":"info","ts":"2024-08-31T21:46:06.007193Z","caller":"traceutil/trace.go:171","msg":"trace[372550948] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:15529; }","duration":"409.549762ms","start":"2024-08-31T21:46:05.59762Z","end":"2024-08-31T21:46:06.00717Z","steps":["trace[372550948] 'agreement among raft nodes before linearized reading'  (duration: 409.415957ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:46:06.007256Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T21:46:05.597608Z","time spent":"409.632966ms","remote":"127.0.0.1:39170","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1134,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-08-31T21:46:06.007264Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T21:46:05.4269Z","time spent":"580.258291ms","remote":"127.0.0.1:39276","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:15521 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-08-31T21:46:06.302444Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"223.004429ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213254017 > lease_revoke:<id:70cc91a92894f548>","response":"size:29"}
{"level":"info","ts":"2024-08-31T21:46:06.302851Z","caller":"traceutil/trace.go:171","msg":"trace[1549596640] linearizableReadLoop","detail":"{readStateIndex:19907; appliedIndex:19906; }","duration":"295.817713ms","start":"2024-08-31T21:46:06.007001Z","end":"2024-08-31T21:46:06.302819Z","steps":["trace[1549596640] 'read index received'  (duration: 72.577876ms)","trace[1549596640] 'applied index is now lower than readState.Index'  (duration: 223.234137ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T21:46:06.302996Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"305.813995ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T21:46:06.303064Z","caller":"traceutil/trace.go:171","msg":"trace[10866483] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:15529; }","duration":"305.905799ms","start":"2024-08-31T21:46:05.997135Z","end":"2024-08-31T21:46:06.303041Z","steps":["trace[10866483] 'agreement among raft nodes before linearized reading'  (duration: 305.799594ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:46:06.303097Z","caller":"traceutil/trace.go:171","msg":"trace[236285681] transaction","detail":"{read_only:false; response_revision:15530; number_of_response:1; }","duration":"288.871747ms","start":"2024-08-31T21:46:06.014208Z","end":"2024-08-31T21:46:06.30308Z","steps":["trace[236285681] 'process raft request'  (duration: 288.588836ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:46:06.303127Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T21:46:05.997123Z","time spent":"305.989802ms","remote":"127.0.0.1:39014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-08-31T21:46:10.523219Z","caller":"traceutil/trace.go:171","msg":"trace[390061647] transaction","detail":"{read_only:false; response_revision:15534; number_of_response:1; }","duration":"112.230202ms","start":"2024-08-31T21:46:10.410972Z","end":"2024-08-31T21:46:10.523202Z","steps":["trace[390061647] 'process raft request'  (duration: 112.100695ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:46:15.998021Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.621483ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213254057 > lease_revoke:<id:70cc91a92894f56d>","response":"size:29"}
{"level":"info","ts":"2024-08-31T21:46:23.00514Z","caller":"traceutil/trace.go:171","msg":"trace[2105046548] transaction","detail":"{read_only:false; response_revision:15544; number_of_response:1; }","duration":"194.358182ms","start":"2024-08-31T21:46:22.81076Z","end":"2024-08-31T21:46:23.005119Z","steps":["trace[2105046548] 'process raft request'  (duration: 194.079788ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:46:30.928989Z","caller":"traceutil/trace.go:171","msg":"trace[213276494] transaction","detail":"{read_only:false; response_revision:15551; number_of_response:1; }","duration":"117.647648ms","start":"2024-08-31T21:46:30.811327Z","end":"2024-08-31T21:46:30.928975Z","steps":["trace[213276494] 'process raft request'  (duration: 117.54314ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:46:45.661625Z","caller":"traceutil/trace.go:171","msg":"trace[1945098630] transaction","detail":"{read_only:false; response_revision:15562; number_of_response:1; }","duration":"119.400317ms","start":"2024-08-31T21:46:45.542198Z","end":"2024-08-31T21:46:45.661599Z","steps":["trace[1945098630] 'process raft request'  (duration: 119.225333ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:46:55.603165Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.19003ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213254212 > lease_revoke:<id:70cc91a92894f609>","response":"size:29"}
{"level":"warn","ts":"2024-08-31T21:47:10.507141Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.916549ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213254276 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:15580 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T21:47:10.50723Z","caller":"traceutil/trace.go:171","msg":"trace[1848913083] transaction","detail":"{read_only:false; response_revision:15582; number_of_response:1; }","duration":"149.655473ms","start":"2024-08-31T21:47:10.357563Z","end":"2024-08-31T21:47:10.507219Z","steps":["trace[1848913083] 'process raft request'  (duration: 28.613922ms)","trace[1848913083] 'compare'  (duration: 120.826646ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T21:47:21.656594Z","caller":"traceutil/trace.go:171","msg":"trace[69742462] transaction","detail":"{read_only:false; response_revision:15591; number_of_response:1; }","duration":"102.666323ms","start":"2024-08-31T21:47:21.553878Z","end":"2024-08-31T21:47:21.656544Z","steps":["trace[69742462] 'process raft request'  (duration: 102.457218ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T21:47:25.605256Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.203182ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031583213254338 > lease_revoke:<id:70cc91a92894f67d>","response":"size:29"}
{"level":"info","ts":"2024-08-31T21:47:27.023487Z","caller":"traceutil/trace.go:171","msg":"trace[783181690] transaction","detail":"{read_only:false; response_revision:15594; number_of_response:1; }","duration":"145.664716ms","start":"2024-08-31T21:47:26.877804Z","end":"2024-08-31T21:47:27.023469Z","steps":["trace[783181690] 'process raft request'  (duration: 145.500317ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:47:37.395272Z","caller":"traceutil/trace.go:171","msg":"trace[1572704944] transaction","detail":"{read_only:false; response_revision:15602; number_of_response:1; }","duration":"116.567592ms","start":"2024-08-31T21:47:37.278685Z","end":"2024-08-31T21:47:37.395252Z","steps":["trace[1572704944] 'process raft request'  (duration: 116.392689ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:47:40.305427Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-08-31T21:47:40.420503Z","caller":"traceutil/trace.go:171","msg":"trace[2023778993] transaction","detail":"{read_only:false; response_revision:15605; number_of_response:1; }","duration":"113.960588ms","start":"2024-08-31T21:47:40.306526Z","end":"2024-08-31T21:47:40.420487Z","steps":["trace[2023778993] 'process raft request'  (duration: 113.853187ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T21:47:40.519654Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2024-08-31T21:47:40.519756Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2024-08-31T21:47:41.558798Z","caller":"traceutil/trace.go:171","msg":"trace[723621569] transaction","detail":"{read_only:false; response_revision:15606; number_of_response:1; }","duration":"104.44008ms","start":"2024-08-31T21:47:41.454341Z","end":"2024-08-31T21:47:41.558781Z","steps":["trace[723621569] 'process raft request'  (duration: 104.313779ms)"],"step_count":1}


==> etcd [38e3c40e7219] <==
{"level":"warn","ts":"2024-08-31T15:56:13.895368Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"508.149129ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786590 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1095 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128031579591786586 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T15:56:13.895489Z","caller":"traceutil/trace.go:171","msg":"trace[758037734] linearizableReadLoop","detail":"{readStateIndex:1299; appliedIndex:1298; }","duration":"194.04132ms","start":"2024-08-31T15:56:13.701431Z","end":"2024-08-31T15:56:13.895473Z","steps":["trace[758037734] 'read index received'  (duration: 42.5s)","trace[758037734] 'applied index is now lower than readState.Index'  (duration: 193.99682ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T15:56:13.895554Z","caller":"traceutil/trace.go:171","msg":"trace[1330883039] transaction","detail":"{read_only:false; response_revision:1102; number_of_response:1; }","duration":"903.633664ms","start":"2024-08-31T15:56:12.991906Z","end":"2024-08-31T15:56:13.895539Z","steps":["trace[1330883039] 'process raft request'  (duration: 395.233635ms)","trace[1330883039] 'compare'  (duration: 507.98593ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:13.89562Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.20202ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-08-31T15:56:13.895656Z","caller":"traceutil/trace.go:171","msg":"trace[1497577210] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1102; }","duration":"194.26412ms","start":"2024-08-31T15:56:13.701384Z","end":"2024-08-31T15:56:13.895648Z","steps":["trace[1497577210] 'agreement among raft nodes before linearized reading'  (duration: 194.13662ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:13.895624Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:12.991895Z","time spent":"903.686764ms","remote":"127.0.0.1:35100","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1095 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128031579591786586 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-08-31T15:56:14.399313Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128031579591786592,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-08-31T15:56:14.685566Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.822213ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786593 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1100 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T15:56:14.685652Z","caller":"traceutil/trace.go:171","msg":"trace[1931137894] transaction","detail":"{read_only:false; response_revision:1103; number_of_response:1; }","duration":"786.740472ms","start":"2024-08-31T15:56:13.898949Z","end":"2024-08-31T15:56:14.685636Z","steps":["trace[1931137894] 'process raft request'  (duration: 584.772659ms)","trace[1931137894] 'compare'  (duration: 201.725313ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:14.685752Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:13.898938Z","time spent":"786.795972ms","remote":"127.0.0.1:35268","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1100 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-08-31T15:56:15.473345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"298.031524ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786594 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:889 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:4345 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T15:56:15.473425Z","caller":"traceutil/trace.go:171","msg":"trace[410099193] linearizableReadLoop","detail":"{readStateIndex:1301; appliedIndex:1299; }","duration":"1.574495343s","start":"2024-08-31T15:56:13.898968Z","end":"2024-08-31T15:56:15.47341Z","steps":["trace[410099193] 'read index received'  (duration: 584.884158ms)","trace[410099193] 'applied index is now lower than readState.Index'  (duration: 989.609585ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T15:56:15.473498Z","caller":"traceutil/trace.go:171","msg":"trace[259840786] transaction","detail":"{read_only:false; response_revision:1104; number_of_response:1; }","duration":"1.182726306s","start":"2024-08-31T15:56:14.290755Z","end":"2024-08-31T15:56:15.473481Z","steps":["trace[259840786] 'process raft request'  (duration: 884.480482ms)","trace[259840786] 'compare'  (duration: 297.920824ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:15.473552Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.574646743s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-08-31T15:56:15.473586Z","caller":"traceutil/trace.go:171","msg":"trace[735267227] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:1104; }","duration":"1.574701643s","start":"2024-08-31T15:56:13.898929Z","end":"2024-08-31T15:56:15.473577Z","steps":["trace[735267227] 'agreement among raft nodes before linearized reading'  (duration: 1.574628843s)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:15.473583Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:14.290739Z","time spent":"1.182794606s","remote":"127.0.0.1:35274","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4379,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/minions/minikube\" mod_revision:889 > success:<request_put:<key:\"/registry/minions/minikube\" value_size:4345 >> failure:<request_range:<key:\"/registry/minions/minikube\" > >"}
{"level":"warn","ts":"2024-08-31T15:56:15.473618Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:13.898919Z","time spent":"1.574742343s","remote":"127.0.0.1:35100","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" "}
{"level":"warn","ts":"2024-08-31T15:56:15.473671Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.146995339s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T15:56:15.473702Z","caller":"traceutil/trace.go:171","msg":"trace[1706531960] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1104; }","duration":"1.147048939s","start":"2024-08-31T15:56:14.326644Z","end":"2024-08-31T15:56:15.473693Z","steps":["trace[1706531960] 'agreement among raft nodes before linearized reading'  (duration: 1.147001139s)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:15.473771Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:14.326626Z","time spent":"1.147135439s","remote":"127.0.0.1:35066","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-08-31T15:56:17.146688Z","caller":"traceutil/trace.go:171","msg":"trace[1873339175] transaction","detail":"{read_only:false; response_revision:1105; number_of_response:1; }","duration":"454.352206ms","start":"2024-08-31T15:56:16.692316Z","end":"2024-08-31T15:56:17.146668Z","steps":["trace[1873339175] 'process raft request'  (duration: 454.211206ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:17.146837Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:16.6923Z","time spent":"454.463806ms","remote":"127.0.0.1:35268","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1103 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-08-31T15:56:18.451278Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.145887163s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786603 > lease_revoke:<id:70cc91a850b9c039>","response":"size:29"}
{"level":"info","ts":"2024-08-31T15:56:18.451382Z","caller":"traceutil/trace.go:171","msg":"trace[440393525] linearizableReadLoop","detail":"{readStateIndex:1303; appliedIndex:1302; }","duration":"1.123656667s","start":"2024-08-31T15:56:17.327709Z","end":"2024-08-31T15:56:18.451366Z","steps":["trace[440393525] 'read index received'  (duration: 55.2s)","trace[440393525] 'applied index is now lower than readState.Index'  (duration: 1.123600067s)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:18.451454Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.123738267s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T15:56:18.451555Z","caller":"traceutil/trace.go:171","msg":"trace[757358989] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1105; }","duration":"1.123879567s","start":"2024-08-31T15:56:17.327664Z","end":"2024-08-31T15:56:18.451544Z","steps":["trace[757358989] 'agreement among raft nodes before linearized reading'  (duration: 1.123746867s)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:18.451596Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:17.32764Z","time spent":"1.123940867s","remote":"127.0.0.1:35066","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-08-31T15:56:19.038244Z","caller":"traceutil/trace.go:171","msg":"trace[543614115] transaction","detail":"{read_only:false; response_revision:1106; number_of_response:1; }","duration":"360.652425ms","start":"2024-08-31T15:56:18.677571Z","end":"2024-08-31T15:56:19.038224Z","steps":["trace[543614115] 'process raft request'  (duration: 360.475125ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:19.038389Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:18.677555Z","time spent":"360.767125ms","remote":"127.0.0.1:35386","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1098 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2024-08-31T15:56:19.491402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"341.22653ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-08-31T15:56:19.491491Z","caller":"traceutil/trace.go:171","msg":"trace[268597139] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1106; }","duration":"341.34333ms","start":"2024-08-31T15:56:19.150131Z","end":"2024-08-31T15:56:19.491475Z","steps":["trace[268597139] 'range keys from in-memory index tree'  (duration: 341.02643ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:19.491538Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:19.150118Z","time spent":"341.40293ms","remote":"127.0.0.1:35268","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":625,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-08-31T15:56:19.607973Z","caller":"traceutil/trace.go:171","msg":"trace[1943435879] transaction","detail":"{read_only:false; response_revision:1107; number_of_response:1; }","duration":"113.482576ms","start":"2024-08-31T15:56:19.494453Z","end":"2024-08-31T15:56:19.607936Z","steps":["trace[1943435879] 'process raft request'  (duration: 113.193477ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:21.816728Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.718774ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786619 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:1102 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128031579591786617 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T15:56:21.816861Z","caller":"traceutil/trace.go:171","msg":"trace[1094299966] linearizableReadLoop","detail":"{readStateIndex:1307; appliedIndex:1306; }","duration":"204.972158ms","start":"2024-08-31T15:56:21.611874Z","end":"2024-08-31T15:56:21.816846Z","steps":["trace[1094299966] 'read index received'  (duration: 81.130284ms)","trace[1094299966] 'applied index is now lower than readState.Index'  (duration: 123.840674ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:21.816989Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.124958ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-08-31T15:56:21.817041Z","caller":"traceutil/trace.go:171","msg":"trace[1529580577] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1108; }","duration":"205.198558ms","start":"2024-08-31T15:56:21.611822Z","end":"2024-08-31T15:56:21.817021Z","steps":["trace[1529580577] 'agreement among raft nodes before linearized reading'  (duration: 205.081158ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T15:56:21.817041Z","caller":"traceutil/trace.go:171","msg":"trace[2048733755] transaction","detail":"{read_only:false; response_revision:1108; number_of_response:1; }","duration":"288.14614ms","start":"2024-08-31T15:56:21.528881Z","end":"2024-08-31T15:56:21.817027Z","steps":["trace[2048733755] 'process raft request'  (duration: 164.063266ms)","trace[2048733755] 'compare'  (duration: 123.586174ms)"],"step_count":2}
{"level":"info","ts":"2024-08-31T15:56:22.772967Z","caller":"traceutil/trace.go:171","msg":"trace[1098845359] transaction","detail":"{read_only:false; response_revision:1110; number_of_response:1; }","duration":"102.948879ms","start":"2024-08-31T15:56:22.669986Z","end":"2024-08-31T15:56:22.772935Z","steps":["trace[1098845359] 'process raft request'  (duration: 102.838879ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T15:56:24.562268Z","caller":"traceutil/trace.go:171","msg":"trace[465752571] transaction","detail":"{read_only:false; response_revision:1111; number_of_response:1; }","duration":"673.295525ms","start":"2024-08-31T15:56:23.888948Z","end":"2024-08-31T15:56:24.562243Z","steps":["trace[465752571] 'process raft request'  (duration: 673.153623ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:24.5624Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:23.888932Z","time spent":"673.413926ms","remote":"127.0.0.1:35268","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1109 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-08-31T15:56:26.775074Z","caller":"traceutil/trace.go:171","msg":"trace[148494634] transaction","detail":"{read_only:false; response_revision:1112; number_of_response:1; }","duration":"204.699848ms","start":"2024-08-31T15:56:26.570359Z","end":"2024-08-31T15:56:26.775059Z","steps":["trace[148494634] 'process raft request'  (duration: 204.575847ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:28.560472Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.015751ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786639 > lease_revoke:<id:70cc91a850b9c05a>","response":"size:29"}
{"level":"info","ts":"2024-08-31T15:56:28.981216Z","caller":"traceutil/trace.go:171","msg":"trace[134634955] transaction","detail":"{read_only:false; response_revision:1113; number_of_response:1; }","duration":"199.876753ms","start":"2024-08-31T15:56:28.781317Z","end":"2024-08-31T15:56:28.981193Z","steps":["trace[134634955] 'process raft request'  (duration: 199.708553ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-31T15:56:29.452109Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.237767ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031579591786642 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1106 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2024-08-31T15:56:29.45223Z","caller":"traceutil/trace.go:171","msg":"trace[950418976] linearizableReadLoop","detail":"{readStateIndex:1314; appliedIndex:1313; }","duration":"124.89687ms","start":"2024-08-31T15:56:29.327315Z","end":"2024-08-31T15:56:29.452212Z","steps":["trace[950418976] 'read index received'  (duration: 26.1s)","trace[950418976] 'applied index is now lower than readState.Index'  (duration: 124.86657ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:29.4523Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.97407ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-08-31T15:56:29.452365Z","caller":"traceutil/trace.go:171","msg":"trace[2017829509] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1114; }","duration":"125.06767ms","start":"2024-08-31T15:56:29.327288Z","end":"2024-08-31T15:56:29.452356Z","steps":["trace[2017829509] 'agreement among raft nodes before linearized reading'  (duration: 124.96937ms)"],"step_count":1}
{"level":"info","ts":"2024-08-31T15:56:29.452358Z","caller":"traceutil/trace.go:171","msg":"trace[484581863] transaction","detail":"{read_only:false; response_revision:1114; number_of_response:1; }","duration":"388.247809ms","start":"2024-08-31T15:56:29.064075Z","end":"2024-08-31T15:56:29.452323Z","steps":["trace[484581863] 'process raft request'  (duration: 246.726842ms)","trace[484581863] 'compare'  (duration: 141.077068ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-31T15:56:29.452449Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-31T15:56:29.064051Z","time spent":"388.345909ms","remote":"127.0.0.1:35386","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1106 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-08-31T15:56:30.73871Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-08-31T15:56:30.738804Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-08-31T15:56:30.73892Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-31T15:56:30.739134Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-31T15:56:30.749654Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-08-31T15:56:30.749703Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-08-31T15:56:30.751119Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-08-31T15:56:31.108309Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-31T15:56:31.108452Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-08-31T15:56:31.108469Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 21:47:49 up 14:07,  0 users,  load average: 0.53, 0.58, 0.62
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [109fbebb4967] <==
W0831 15:56:36.448437       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:36.454487       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:36.591357       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:36.640486       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:37.364366       1 logging.go:59] [core] [Channel #256 SubChannel #257] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:38.915060       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:38.992017       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.123259       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.220423       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.290547       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.294199       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.306483       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.321757       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.372502       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.377190       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.432188       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.464352       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.484694       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.515777       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.521277       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.529572       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.539179       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.540467       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.549906       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.562626       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.574853       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.702483       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.720100       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.732568       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.794850       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.824719       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.828814       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.831507       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.879389       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.887432       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.897550       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.910330       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.912667       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.939958       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.950551       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:39.976904       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.006741       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.009017       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.028308       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.037676       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.080252       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.090829       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.120428       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.162271       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.163851       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.209817       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.215360       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.246102       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.258667       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.474711       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.521140       1 logging.go:59] [core] [Channel #15 SubChannel #17] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.659551       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.664100       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.728578       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0831 15:56:40.742840       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [351f98c58139] <==
I0831 21:39:05.843105       1 trace.go:236] Trace[312928888]: "Update" accept:application/json, */*,audit-id:c8fc7499-6fcf-4371-8fda-fdfc936f9912,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Aug-2024 21:39:05.151) (total time: 691ms):
Trace[312928888]: ["GuaranteedUpdate etcd3" audit-id:c8fc7499-6fcf-4371-8fda-fdfc936f9912,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 691ms (21:39:05.151)
Trace[312928888]:  ---"Txn call completed" 690ms (21:39:05.842)]
Trace[312928888]: [691.700187ms] [691.700187ms] END
I0831 21:41:30.953719       1 trace.go:236] Trace[1991550522]: "Update" accept:application/json, */*,audit-id:dbfc07c4-b6d0-4820-a77f-c9b56b027bd0,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Aug-2024 21:41:30.270) (total time: 683ms):
Trace[1991550522]: ["GuaranteedUpdate etcd3" audit-id:dbfc07c4-b6d0-4820-a77f-c9b56b027bd0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 683ms (21:41:30.270)
Trace[1991550522]:  ---"Txn call completed" 682ms (21:41:30.953)]
Trace[1991550522]: [683.257053ms] [683.257053ms] END
I0831 21:41:31.234115       1 trace.go:236] Trace[272564177]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (31-Aug-2024 21:41:30.282) (total time: 951ms):
Trace[272564177]: ---"initial value restored" 671ms (21:41:30.953)
Trace[272564177]: ---"Transaction prepared" 74ms (21:41:31.028)
Trace[272564177]: ---"Txn call completed" 205ms (21:41:31.234)
Trace[272564177]: [951.606035ms] [951.606035ms] END
I0831 21:42:00.856166       1 trace.go:236] Trace[2005950191]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (31-Aug-2024 21:42:00.284) (total time: 571ms):
Trace[2005950191]: ---"initial value restored" 110ms (21:42:00.395)
Trace[2005950191]: ---"Transaction prepared" 188ms (21:42:00.584)
Trace[2005950191]: ---"Txn call completed" 272ms (21:42:00.856)
Trace[2005950191]: [571.295947ms] [571.295947ms] END
I0831 21:42:20.249910       1 trace.go:236] Trace[72655639]: "Update" accept:application/json, */*,audit-id:63079c10-921e-49e6-9901-09c2999944c3,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Aug-2024 21:42:19.712) (total time: 537ms):
Trace[72655639]: ["GuaranteedUpdate etcd3" audit-id:63079c10-921e-49e6-9901-09c2999944c3,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 536ms (21:42:19.713)
Trace[72655639]:  ---"Txn call completed" 536ms (21:42:20.249)]
Trace[72655639]: [537.045304ms] [537.045304ms] END
I0831 21:42:21.170854       1 trace.go:236] Trace[1560026936]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (31-Aug-2024 21:42:20.286) (total time: 884ms):
Trace[1560026936]: ---"initial value restored" 207ms (21:42:20.494)
Trace[1560026936]: ---"Transaction prepared" 156ms (21:42:20.650)
Trace[1560026936]: ---"Txn call completed" 520ms (21:42:21.170)
Trace[1560026936]: [884.547408ms] [884.547408ms] END
I0831 21:42:27.737833       1 trace.go:236] Trace[1496137328]: "Update" accept:application/json, */*,audit-id:c45797ea-5385-4483-a362-148965c957fe,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Aug-2024 21:42:26.977) (total time: 760ms):
Trace[1496137328]: ["GuaranteedUpdate etcd3" audit-id:c45797ea-5385-4483-a362-148965c957fe,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 759ms (21:42:26.977)
Trace[1496137328]:  ---"Txn call completed" 758ms (21:42:27.737)]
Trace[1496137328]: [760.392926ms] [760.392926ms] END
I0831 21:42:27.947950       1 trace.go:236] Trace[630331239]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:794cdfa0-764f-42d7-9faf-015a72904a49,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (31-Aug-2024 21:42:27.211) (total time: 736ms):
Trace[630331239]: ["GuaranteedUpdate etcd3" audit-id:794cdfa0-764f-42d7-9faf-015a72904a49,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 735ms (21:42:27.212)
Trace[630331239]:  ---"Txn call completed" 734ms (21:42:27.947)]
Trace[630331239]: [736.009225ms] [736.009225ms] END
I0831 21:43:50.258321       1 trace.go:236] Trace[2141332854]: "Update" accept:application/json, */*,audit-id:acea97dd-2920-49a7-ad5f-7a643cbf9ba5,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (31-Aug-2024 21:43:49.881) (total time: 585ms):
Trace[2141332854]: ["GuaranteedUpdate etcd3" audit-id:acea97dd-2920-49a7-ad5f-7a643cbf9ba5,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 585ms (21:43:49.882)
Trace[2141332854]:  ---"Txn call completed" 584ms (21:43:50.258)]
Trace[2141332854]: [585.446284ms] [585.446284ms] END
I0831 21:43:51.213627       1 trace.go:236] Trace[827124744]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5149e81a-a2d6-48cb-82b9-bc9367ee7824,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (31-Aug-2024 21:43:49.923) (total time: 1499ms):
Trace[827124744]: ["GuaranteedUpdate etcd3" audit-id:5149e81a-a2d6-48cb-82b9-bc9367ee7824,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1499ms (21:43:49.923)
Trace[827124744]:  ---"Txn call completed" 1498ms (21:43:51.213)]
Trace[827124744]: [1.499288149s] [1.499288149s] END
I0831 21:43:51.535374       1 trace.go:236] Trace[11508149]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (31-Aug-2024 21:43:50.083) (total time: 1452ms):
Trace[11508149]: ---"initial value restored" 1130ms (21:43:51.213)
Trace[11508149]: ---"Transaction prepared" 74ms (21:43:51.287)
Trace[11508149]: ---"Txn call completed" 247ms (21:43:51.535)
Trace[11508149]: [1.452021368s] [1.452021368s] END
I0831 21:44:10.811334       1 trace.go:236] Trace[148071853]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (31-Aug-2024 21:44:10.084) (total time: 726ms):
Trace[148071853]: ---"Transaction prepared" 271ms (21:44:10.357)
Trace[148071853]: ---"Txn call completed" 453ms (21:44:10.811)
Trace[148071853]: [726.481672ms] [726.481672ms] END
I0831 21:44:12.519633       1 trace.go:236] Trace[2129844838]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c4c02ae1-41da-4420-8889-8ad5f1f690a1,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (31-Aug-2024 21:44:11.846) (total time: 673ms):
Trace[2129844838]: ["GuaranteedUpdate etcd3" audit-id:c4c02ae1-41da-4420-8889-8ad5f1f690a1,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 673ms (21:44:11.846)
Trace[2129844838]:  ---"Txn call completed" 672ms (21:44:12.519)]
Trace[2129844838]: [673.579852ms] [673.579852ms] END
I0831 21:46:06.008401       1 trace.go:236] Trace[1624662740]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:63267bb8-a943-462e-8d4c-047a3a76ef99,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (31-Aug-2024 21:46:05.425) (total time: 582ms):
Trace[1624662740]: ["GuaranteedUpdate etcd3" audit-id:63267bb8-a943-462e-8d4c-047a3a76ef99,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 582ms (21:46:05.425)
Trace[1624662740]:  ---"Txn call completed" 581ms (21:46:06.008)]
Trace[1624662740]: [582.592881ms] [582.592881ms] END


==> kube-controller-manager [055341c90545] <==
I0831 19:17:14.008957       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="113.301s"
I0831 19:17:25.005761       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.701s"
I0831 19:22:25.038563       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="89.201s"
I0831 19:22:38.943535       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="116.001s"
I0831 19:27:23.951124       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.4s"
I0831 19:27:37.958401       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="79.301s"
I0831 19:33:19.633525       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="65.4s"
I0831 19:33:47.211547       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="110.501s"
I0831 19:38:22.437289       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="118.7s"
I0831 19:38:35.124685       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="111.7s"
I0831 19:43:35.122053       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="177.3s"
I0831 19:43:46.569057       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="97.501s"
I0831 19:48:41.333309       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="116.5s"
I0831 19:48:53.069355       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="102.3s"
I0831 19:53:42.066799       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="99.3s"
I0831 19:53:56.337237       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="115.1s"
I0831 19:58:50.969341       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="72.18s"
I0831 19:59:02.938083       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="101.892s"
I0831 20:03:51.969566       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="131.2s"
I0831 20:04:04.951963       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="123.6s"
I0831 20:08:57.054069       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.5s"
I0831 20:09:09.032964       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="56.9s"
I0831 20:14:11.634340       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="87.901s"
I0831 20:14:19.470052       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="93.2s"
I0831 20:19:17.992752       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="91.3s"
I0831 20:19:29.911777       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="96s"
I0831 20:24:27.047947       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="80.301s"
I0831 20:24:38.968096       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="64s"
I0831 20:29:35.999216       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="92.901s"
I0831 20:29:47.987930       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="95s"
I0831 20:34:46.996792       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.201s"
I0831 20:35:00.361221       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="63.5s"
I0831 20:39:52.998834       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="99.4s"
I0831 20:40:08.349919       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="50.3s"
I0831 20:44:58.950713       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="108.301s"
I0831 20:45:13.980101       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="119.4s"
I0831 20:50:04.258061       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="112.799s"
I0831 20:50:18.212176       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="110.3s"
I0831 20:55:07.004339       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="104.7s"
I0831 20:55:18.093345       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="110.5s"
I0831 21:00:11.909827       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="100.3s"
I0831 21:00:25.079762       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="115.301s"
I0831 21:05:21.951126       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="103.9s"
I0831 21:05:36.980840       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="83.5s"
I0831 21:10:29.313051       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="109.8s"
I0831 21:10:43.370106       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="122.6s"
I0831 21:15:37.095125       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.2s"
I0831 21:15:49.024637       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="107.5s"
I0831 21:20:42.269573       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="271.5s"
I0831 21:20:55.108571       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="121.5s"
I0831 21:25:50.095712       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="94.801s"
I0831 21:26:04.516134       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="56.601s"
I0831 21:31:01.302368       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="77.8s"
I0831 21:31:14.467358       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="93.4s"
I0831 21:36:04.061750       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="96.001s"
I0831 21:36:16.147617       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.301s"
I0831 21:41:14.951956       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="98.4s"
I0831 21:41:27.195299       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="61.1s"
I0831 21:46:17.871613       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="92.404s"
I0831 21:46:30.930315       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/customer-model-57dd76499c" duration="113.31s"


==> kube-proxy [1fcc1c1d2522] <==
I0831 12:03:34.572729       1 server_linux.go:69] "Using iptables proxy"
I0831 12:03:34.620920       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0831 12:03:34.640926       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0831 12:03:34.640983       1 server_linux.go:165] "Using iptables Proxier"
I0831 12:03:34.642889       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0831 12:03:34.642927       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0831 12:03:34.642950       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0831 12:03:34.643308       1 server.go:872] "Version info" version="v1.30.0"
I0831 12:03:34.643361       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 12:03:34.644311       1 config.go:192] "Starting service config controller"
I0831 12:03:34.644365       1 shared_informer.go:313] Waiting for caches to sync for service config
I0831 12:03:34.644438       1 config.go:101] "Starting endpoint slice config controller"
I0831 12:03:34.644480       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0831 12:03:34.644463       1 config.go:319] "Starting node config controller"
I0831 12:03:34.644931       1 shared_informer.go:313] Waiting for caches to sync for node config
I0831 12:03:34.744593       1 shared_informer.go:320] Caches are synced for service config
I0831 12:03:34.744635       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0831 12:03:34.745242       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [7e49007da47e] <==
I0831 15:58:15.845614       1 server_linux.go:69] "Using iptables proxy"
I0831 15:58:19.723624       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0831 15:58:21.688539       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0831 15:58:21.688620       1 server_linux.go:165] "Using iptables Proxier"
I0831 15:58:21.690820       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0831 15:58:21.690861       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0831 15:58:21.690877       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0831 15:58:21.691219       1 server.go:872] "Version info" version="v1.30.0"
I0831 15:58:21.691280       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 15:58:21.692240       1 config.go:319] "Starting node config controller"
I0831 15:58:21.692267       1 shared_informer.go:313] Waiting for caches to sync for node config
I0831 15:58:21.692513       1 config.go:101] "Starting endpoint slice config controller"
I0831 15:58:21.692549       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0831 15:58:21.692578       1 config.go:192] "Starting service config controller"
I0831 15:58:21.692585       1 shared_informer.go:313] Waiting for caches to sync for service config
I0831 15:58:21.793043       1 shared_informer.go:320] Caches are synced for service config
I0831 15:58:21.793105       1 shared_informer.go:320] Caches are synced for node config
I0831 15:58:21.793082       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [54fd5882840e] <==
W0831 12:02:31.295112       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0831 12:02:31.295157       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0831 12:02:31.295306       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:31.295437       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:31.295605       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0831 12:02:31.295640       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0831 12:02:33.272536       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0831 12:02:33.272586       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0831 12:02:34.507521       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0831 12:02:34.507572       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0831 12:02:34.686027       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0831 12:02:34.686115       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0831 12:02:34.979724       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0831 12:02:34.979790       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0831 12:02:35.332089       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0831 12:02:35.332214       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0831 12:02:35.537444       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0831 12:02:35.537506       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0831 12:02:35.643143       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0831 12:02:35.643194       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0831 12:02:35.925066       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0831 12:02:35.925152       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0831 12:02:36.320387       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:36.320437       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:36.676971       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:36.677023       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:36.687615       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:36.687689       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:37.079080       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0831 12:02:37.079146       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0831 12:02:37.221980       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:37.222104       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:37.340698       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0831 12:02:37.340773       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0831 12:02:37.389315       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0831 12:02:37.389364       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0831 12:02:42.159055       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0831 12:02:42.159103       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0831 12:02:43.127469       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0831 12:02:43.127551       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0831 12:02:43.710108       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0831 12:02:43.710158       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0831 12:02:44.466716       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0831 12:02:44.466764       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0831 12:02:45.533437       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0831 12:02:45.533487       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0831 12:02:45.566973       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0831 12:02:45.567026       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0831 12:02:45.584185       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0831 12:02:45.584231       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0831 12:02:45.833822       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0831 12:02:45.833875       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0831 12:02:46.501571       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:46.501620       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:46.557004       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0831 12:02:46.557054       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0831 12:02:46.685176       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0831 12:02:46.685228       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I0831 12:02:57.044024       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0831 15:56:30.732962       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [6f35480baee2] <==
I0831 15:58:14.689345       1 serving.go:380] Generated self-signed cert in-memory
W0831 15:58:19.066212       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0831 15:58:19.066731       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0831 15:58:19.067469       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0831 15:58:19.067690       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0831 15:58:19.082267       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0831 15:58:19.082304       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0831 15:58:19.083768       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0831 15:58:19.083824       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0831 15:58:19.083973       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0831 15:58:19.083995       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0831 15:58:19.125840       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0831 15:58:19.125928       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0831 15:58:19.128849       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0831 15:58:19.128988       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0831 15:58:19.129048       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0831 15:58:19.129148       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0831 15:58:19.129207       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0831 15:58:19.129247       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0831 15:58:19.129359       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0831 15:58:19.129403       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0831 15:58:19.129917       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0831 15:58:19.130041       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0831 15:58:19.130090       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0831 15:58:19.130040       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0831 15:58:19.129973       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0831 15:58:19.130165       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0831 15:58:19.130206       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0831 15:58:19.130213       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0831 15:58:19.130249       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0831 15:58:19.130300       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0831 15:58:19.130417       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0831 15:58:19.130458       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0831 15:58:19.131398       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0831 15:58:19.131425       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0831 15:58:19.131477       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0831 15:58:19.131477       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0831 15:58:19.131488       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0831 15:58:19.131637       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0831 15:58:19.132329       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0831 15:58:19.132413       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I0831 15:58:20.384884       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Aug 31 21:44:26 minikube kubelet[2527]: E0831 21:44:26.110967    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21692338 maxSize=10485760
Aug 31 21:44:28 minikube kubelet[2527]: E0831 21:44:28.858153    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:44:36 minikube kubelet[2527]: E0831 21:44:36.114332    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:44:36 minikube kubelet[2527]: E0831 21:44:36.114429    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21697113 maxSize=10485760
Aug 31 21:44:41 minikube kubelet[2527]: E0831 21:44:41.856500    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:44:46 minikube kubelet[2527]: E0831 21:44:46.117498    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:44:46 minikube kubelet[2527]: E0831 21:44:46.117603    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21699098 maxSize=10485760
Aug 31 21:44:55 minikube kubelet[2527]: E0831 21:44:55.815043    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:44:56 minikube kubelet[2527]: E0831 21:44:56.079098    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:44:56 minikube kubelet[2527]: E0831 21:44:56.079207    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21704302 maxSize=10485760
Aug 31 21:45:06 minikube kubelet[2527]: E0831 21:45:06.084061    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:06 minikube kubelet[2527]: E0831 21:45:06.084166    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21707747 maxSize=10485760
Aug 31 21:45:09 minikube kubelet[2527]: E0831 21:45:09.814058    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:45:16 minikube kubelet[2527]: E0831 21:45:16.107670    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:16 minikube kubelet[2527]: E0831 21:45:16.107806    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21708158 maxSize=10485760
Aug 31 21:45:23 minikube kubelet[2527]: E0831 21:45:23.807884    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:45:26 minikube kubelet[2527]: E0831 21:45:26.086221    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:26 minikube kubelet[2527]: E0831 21:45:26.086349    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21713302 maxSize=10485760
Aug 31 21:45:36 minikube kubelet[2527]: E0831 21:45:36.094259    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:36 minikube kubelet[2527]: E0831 21:45:36.094369    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21713806 maxSize=10485760
Aug 31 21:45:38 minikube kubelet[2527]: E0831 21:45:38.806707    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:45:46 minikube kubelet[2527]: E0831 21:45:46.096329    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:46 minikube kubelet[2527]: E0831 21:45:46.096430    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21714366 maxSize=10485760
Aug 31 21:45:49 minikube kubelet[2527]: E0831 21:45:49.809387    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:45:56 minikube kubelet[2527]: E0831 21:45:56.098774    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:45:56 minikube kubelet[2527]: E0831 21:45:56.153337    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21717938 maxSize=10485760
Aug 31 21:46:02 minikube kubelet[2527]: E0831 21:46:02.561340    2527 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for customer-model-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="customer-model-app:latest"
Aug 31 21:46:02 minikube kubelet[2527]: E0831 21:46:02.561596    2527 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for customer-model-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="customer-model-app:latest"
Aug 31 21:46:02 minikube kubelet[2527]: E0831 21:46:02.561902    2527 kuberuntime_manager.go:1256] container &Container{Name:customer-model,Image:customer-model-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8089,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:SPRING_DATASOURCE_URL,Value:jdbc:postgresql://postgres:5432/postgres,ValueFrom:nil,},EnvVar{Name:SPRING_DATASOURCE_USERNAME,Value:postgres,ValueFrom:nil,},EnvVar{Name:SPRING_DATASOURCE_PASSWORD,Value:password,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvr9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod customer-model-57dd76499c-xjqtt_default(8ec8526d-067f-4096-88c8-0c4db43d8b9c): ErrImagePull: Error response from daemon: pull access denied for customer-model-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 31 21:46:02 minikube kubelet[2527]: E0831 21:46:02.561964    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ErrImagePull: \"Error response from daemon: pull access denied for customer-model-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:46:06 minikube kubelet[2527]: E0831 21:46:06.103177    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:06 minikube kubelet[2527]: E0831 21:46:06.103281    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21722433 maxSize=10485760
Aug 31 21:46:16 minikube kubelet[2527]: E0831 21:46:16.142843    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:16 minikube kubelet[2527]: E0831 21:46:16.143179    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21726270 maxSize=10485760
Aug 31 21:46:17 minikube kubelet[2527]: E0831 21:46:17.806095    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:46:26 minikube kubelet[2527]: E0831 21:46:26.114421    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:26 minikube kubelet[2527]: E0831 21:46:26.130564    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21726772 maxSize=10485760
Aug 31 21:46:30 minikube kubelet[2527]: E0831 21:46:30.806130    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:46:36 minikube kubelet[2527]: E0831 21:46:36.119770    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:36 minikube kubelet[2527]: E0831 21:46:36.119884    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21727273 maxSize=10485760
Aug 31 21:46:44 minikube kubelet[2527]: E0831 21:46:44.804778    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:46:46 minikube kubelet[2527]: E0831 21:46:46.124406    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:46 minikube kubelet[2527]: E0831 21:46:46.124522    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21727777 maxSize=10485760
Aug 31 21:46:56 minikube kubelet[2527]: E0831 21:46:56.128072    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:46:56 minikube kubelet[2527]: E0831 21:46:56.128173    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21728187 maxSize=10485760
Aug 31 21:46:58 minikube kubelet[2527]: E0831 21:46:58.804063    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:47:06 minikube kubelet[2527]: E0831 21:47:06.143119    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:47:06 minikube kubelet[2527]: E0831 21:47:06.143268    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21728187 maxSize=10485760
Aug 31 21:47:12 minikube kubelet[2527]: E0831 21:47:12.807851    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:47:16 minikube kubelet[2527]: E0831 21:47:16.141923    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:47:16 minikube kubelet[2527]: E0831 21:47:16.142070    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21729583 maxSize=10485760
Aug 31 21:47:24 minikube kubelet[2527]: E0831 21:47:24.803171    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:47:26 minikube kubelet[2527]: E0831 21:47:26.144594    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:47:26 minikube kubelet[2527]: E0831 21:47:26.144712    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21730494 maxSize=10485760
Aug 31 21:47:35 minikube kubelet[2527]: E0831 21:47:35.803280    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"
Aug 31 21:47:36 minikube kubelet[2527]: E0831 21:47:36.162166    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:47:36 minikube kubelet[2527]: E0831 21:47:36.162322    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21730996 maxSize=10485760
Aug 31 21:47:46 minikube kubelet[2527]: E0831 21:47:46.154827    2527 remote_runtime.go:751] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f"
Aug 31 21:47:46 minikube kubelet[2527]: E0831 21:47:46.154932    2527 container_log_manager.go:304] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log\": failed to reopen container log \"20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="20e3951a13c2167fbfbfdc908e930d34cd0cb49022d9793cd926aed9a91c2f7f" path="/var/log/pods/kube-system_etcd-minikube_063d6b9688927e601f52fd818d1305c5/etcd/1.log" currentSize=21733312 maxSize=10485760
Aug 31 21:47:47 minikube kubelet[2527]: E0831 21:47:47.803993    2527 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"customer-model\" with ImagePullBackOff: \"Back-off pulling image \\\"customer-model-app:latest\\\"\"" pod="default/customer-model-57dd76499c-xjqtt" podUID="8ec8526d-067f-4096-88c8-0c4db43d8b9c"


==> storage-provisioner [5fe05e037241] <==
I0831 19:33:40.078419       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0831 19:33:40.087026       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0831 19:33:40.087113       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0831 19:34:04.673905       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0831 19:34:04.674114       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_800acef0-036d-4bf9-b362-1a88852c8d31!
I0831 19:34:04.674348       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"dbc21dfe-66c6-4989-b233-6115e7b12e12", APIVersion:"v1", ResourceVersion:"9514", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_800acef0-036d-4bf9-b362-1a88852c8d31 became leader
I0831 19:34:04.775130       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_800acef0-036d-4bf9-b362-1a88852c8d31!


==> storage-provisioner [69b0f82a5e77] <==
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003ad080, 0x18b3d60, 0xc0005b4000, 0x1, 0xc0001c64e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003ad080, 0x3b9aca00, 0x0, 0x1, 0xc0001c64e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003ad080, 0x3b9aca00, 0xc0001c64e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 3011 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc0002b9bc0, 0x6)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0002b9bb0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0002b9ba8, 0xc00026e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc0002b9b80, 0xc00026e001, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0002b9e40, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0002b9e40, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0002b9e40, 0x154a160, 0xc00069b8f0, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0003a7e00, 0xc000151c00, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc00069ec80, 0x0, 0x18bc168, 0xc0006a6d00, 0x0, 0x0, 0x461dc0, 0xc00055ed20, 0xc000113e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc00030c580, 0xc000113ef0, 0x8, 0x18baa48, 0xc00017a540, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc0003dc700)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 2956 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0xc0005eb0c0, 0x6)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0005eb0b0)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc0005eb0a8, 0xc00026f201, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc0005eb080, 0xc00026f201, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0005eb340, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0005eb340, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0005eb340, 0x154a160, 0xc0004c0f48, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0003885d0, 0xc000207000, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000142370, 0x0, 0x18bc168, 0xc0005a2a40, 0x0, 0x0, 0x461dc0, 0xc0000ee2a0, 0xc0000f5e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc0005481c0, 0xc0000f5ef0, 0x8, 0x18bbba0, 0xc00037ef00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc0005a2340)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

